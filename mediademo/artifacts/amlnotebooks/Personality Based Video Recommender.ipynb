{
  "cells": [
  {
      "cell_type": "markdown",
      "source": [
        "# Disclaimer\n",
        "By accessing this code, you acknowledge the code is made available for presentation and demonstration purposes only and that the code: (1) is not subject to SOC 1 and SOC 2 compliance audits; (2) is not designed or intended to be a substitute for the professional advice, diagnosis, treatment, or judgment of a certified financial services professional; (3) is not designed, intended or made available as a medical device; and (4) is not designed or intended to be a substitute for professional medical advice, diagnosis, treatment or judgement. Do not use this code to replace, substitute, or provide professional financial advice or judgment, or to replace, substitute or provide medical advice, diagnosis, treatment or judgement. You are solely responsible for ensuring the regulatory, legal, and/or contractual compliance of any use of the code, including obtaining any authorizations or consents, and any solution you choose to build that incorporates this code in whole or in part."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a personalized recommender model\r\n",
        "\r\n",
        "#### This notebook builds a video recommendation model using personalized profiles"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import string\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from scipy.sparse import find\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import json"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1616520329644
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading datasets for cleaning text data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520329824
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining folder paths to video files"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona_path= os.path.join(os.getcwd(),\"personas\")\r\n",
        "persona_path = os.path.join(persona_path,\"personas.json\")\r\n",
        "transcripts_location = os.path.join(os.getcwd(),'video_files')"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520329975
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining function to tokenize video transcripts"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\r\n",
        "    tokens = nltk.word_tokenize(text)\r\n",
        "    stems = []\r\n",
        "    for item in tokens:\r\n",
        "        stems.append(nltk.PorterStemmer().stem(item))\r\n",
        "    return stems"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520330211
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing all files with our video data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,file_names = next(os.walk(transcripts_location))\r\n",
        "transcript_files = {}\r\n",
        "raw_text = []\r\n",
        "file_index_mapping = {}\r\n",
        "original_transcript = []\r\n",
        "ids = []\r\n",
        "\r\n",
        "# Going over all files in the chosen directory\r\n",
        "for fname in file_names:\r\n",
        "    json_file = os.path.join(transcripts_location,fname)\r\n",
        "    text = '' \r\n",
        "    \r\n",
        "    with open(json_file) as f: \r\n",
        "        json_obj = json.load(f)\r\n",
        "\r\n",
        "    text = json_obj['transcript']\r\n",
        "    words = text.split()\r\n",
        "    transcript_files[fname] = words\r\n",
        "    raw_text.append(words)\r\n",
        "    original_transcript.append(text)\r\n",
        "    file_index_mapping[json_obj['video_id']] = fname\r\n",
        "    ids.append(json_obj['video_id'])"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520332090
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering stop words from video transcript"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_english = stopwords.words('english')\r\n",
        "\r\n",
        "# Removing stop words from video transcripts\r\n",
        "for i in range(len(ids)):\r\n",
        "    raw_text[i] = [word.lower() for word in raw_text[i]]\r\n",
        "    raw_text[i] = [word for word in raw_text[i] if word not in string.punctuation]\r\n",
        "    raw_text[i] = [word for word in raw_text[i] if word not in stopwords_english]\r\n",
        "    corresponding_file = file_index_mapping[ids[i]]\r\n",
        "    transcript_files[corresponding_file] = raw_text[i]\r\n",
        "raw_text = [' '.join(i) for i in raw_text]"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520332315
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting bag of words from persona files"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "personas = []\r\n",
        "with open(persona_path) as persona_data: \r\n",
        "    persona_obj = json.load(persona_data)\r\n",
        "\r\n",
        "    for i in persona_obj['items']: \r\n",
        "        personas.append(i)\r\n",
        "\r\n",
        "personas[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "{'id': '001',\n 'name': 'Reta',\n 'words': ['machine',\n  'learning',\n  'data',\n  'model',\n  'experiments',\n  'algorithm',\n  'classification',\n  'learning',\n  'analytic',\n  'queries',\n  'databases',\n  'Azure',\n  'Cosmos',\n  'query',\n  'function',\n  'SQL',\n  'distributed',\n  'network',\n  'vitual',\n  'server']}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520332524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persona_words = []\r\n",
        "for persona_obj in personas:\r\n",
        "    words = ' '.join(persona_obj['words'])\r\n",
        "    persona_words.append(words)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520332622
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenize)\r\n",
        "video_tfidf = tfidf.fit_transform(raw_text)\r\n",
        "persona_tfidf = tfidf.transform(persona_words)\r\n",
        "cos_sim = cosine_similarity(persona_tfidf,video_tfidf)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520334822
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining function for fetching recommendations"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommender(persona_index,similarity = cos_sim,topk=5):\r\n",
        "    recommended = []\r\n",
        "    inds = np.argsort(-1*cos_sim[persona_index])[:topk+1]     \r\n",
        "    for i in inds: \r\n",
        "        recommended.append(ids[i])\r\n",
        "    return recommended"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520334898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommendations for Reta"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reta_recommendations_ids = recommender(0)\r\n",
        "reta_recommendations_names = []\r\n",
        "\r\n",
        "for i in reta_recommendations_ids: \r\n",
        "    fname = str(i)+\".json\"\r\n",
        "    file_path = os.path.join(transcripts_location,fname)\r\n",
        "    with open(file_path) as file_data: \r\n",
        "        json_obj = json.load(file_data)\r\n",
        "    reta_recommendations_names.append(json_obj['name'])\r\n",
        "\r\n",
        "reta_recommendations_names"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "['Azure Machine Learning service pipelines in Azure Data Factory.mp4',\n 'Performance tuning and troubleshooting - Azure SQL Data Warehouse.mp4',\n 'Introduction to Azure Cosmos DB Use Cases.mp4',\n 'Why you should modernize to SQL Server 2017.mp4',\n 'SQL Server 2017 for developers and machine learning.mp4',\n 'Learn about Spark and Azure Cosmos DB integration and use case.mp4']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520335051
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommendations for Ryan"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ryan_recommendations_ids = recommender(1)\r\n",
        "ryan_recommendations_names = []\r\n",
        "\r\n",
        "for i in ryan_recommendations_ids: \r\n",
        "    fname = str(i)+\".json\"\r\n",
        "    file_path = os.path.join(transcripts_location,fname)\r\n",
        "    with open(file_path) as file_data: \r\n",
        "        json_obj = json.load(file_data)\r\n",
        "    ryan_recommendations_names.append(json_obj['name'])\r\n",
        "\r\n",
        "ryan_recommendations_names"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "['Every second counts _ Microsoft In Culture.mp4',\n 'Petersen Automotive Museum_ a HoloLens experience.mp4',\n 'INCEPTION trailer.mp4',\n 'Top Gun - Maverick.mp4',\n 'Microsoft Rewards Ultimate Racing Experience.mp4',\n 'Toyota Gazoo Racing.mp4']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520335184
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the model for later use"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(cos_sim).to_csv('personalised_similarity.csv',header=ids,index=False)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616520335302
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}