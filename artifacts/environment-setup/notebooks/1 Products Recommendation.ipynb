{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size:25px; color:black;\"><u><i><b>Product Recommendations</b></i></u></p>\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Product recommendations is a filtering system that predicts and shows the items that a user would likely purchase based on their purchase history.\n",
        "</p>\n",
        "\n",
        "<p style=\"font-size:15px; color:#318f50;\">\n",
        "Note:\n",
        "</p>\n",
        "<p style=\"font-size:15px; color:#117d30;\">\n",
        " This notebook is written in Scala, and there is interoperability between Scala and Python code.\n",
        "</p>\n",
        "<p style=\"font-size:15px; color:#117d30;\">\n",
        "    <u> Steps: </u>\n",
        "</p>\n",
        "<p style=\"font-size:15px; color:#117d30;\">\n",
        "1) Data is ingested from Azure Synapse Data Warehouse using PySpark.\n",
        "</p>\n",
        "<p style=\"font-size:15px; color:#117d30;\">\n",
        "2) The model is trained using the PySpark ML-Lib recommendations module.\n",
        "</p>\n",
        "<p style=\"font-size:15px; color:#117d30;\">\n",
        "3) Product recommendations are generated for the user.\n",
        "</p>"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Connecting to Azure Synapse Data Warehouse*\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Connection to Azure Synapse Data Warehouse is initiated and the required data is ingested for processing.\n",
        "    The warehouse is connected with a single line of code. Just point to actions in a table, click on a new notebook, and then click on \"Load to DataFrame\".  </p>\n",
        "   <p style=\"font-size:16px; color:#117d30;\"> After providing the necessary details,  the required data is loaded in the form of a Spark dataframe.\n",
        "One magical line of code converts a dataframe from Scala to Python!\n",
        "</p>"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "from pyspark import SparkContext\n",
        "\n",
        "import traceback"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "df: org.apache.spark.sql.DataFrame = [customer_id: int, product_id: int ... 3 more fields]\nres1: Array[org.apache.spark.sql.Row] = Array([69145,4,Brown SurfBoard,1,0], [40853,6,Blue Stripped SurfBoard,2,1], [26885,30,Brown Shoes,4,5], [72911,5,Orange SurfBoard,2,1], [16854,4,Brown SurfBoard,4,5], [59638,18,French cheese,4,5], [67347,23,Crystal Wineglass,2,1], [59638,25,White Shoes,1,0], [45041,9,Wine corkscrew,1,0], [72027,9,Wine corkscrew,2,1])"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%spark\n",
        "val df = spark.read.sqlanalytics(\"SQLPool01.dbo.Customer_SalesLatest\") \n",
        "  df.head(10)\n",
        "  //Create a Temp view for using the dataframe from Scala to Python\n",
        "  df.createOrReplaceTempView (\"df\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "status": "error",
          "execution_count": 5,
          "data": null,
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "NameError : name 'df' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'df' is not defined\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "print(df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "status": "error",
          "execution_count": 6,
          "data": null,
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "NameError : name 'df' is not defined",
            "Traceback (most recent call last):\n",
            "NameError: name 'df' is not defined\n"
          ]
        }
      ],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "rating"
            ],
            "values": [
              "customer_id"
            ],
            "yLabel": "customer_id",
            "xLabel": "rating",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "customer_id": {
              "0": 15840248,
              "1": 12289037,
              "3": 14460213,
              "5": 14908088
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "2.4.4.dev0\nNone"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "import pyspark \n",
        "print(print(pyspark.__version__)) "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "from pyspark import SparkContext\n",
        "\n",
        "import traceback"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "#Calling the dataframe df created in Scala to Python\n",
        "df = sqlContext.table(\"df\")\n",
        "# *********************\n",
        "\n",
        "\n",
        "Customer_data = df.select(\"customer_id\", \"product_id\", \"rating\")\n",
        "\n",
        "_toExplore = df.select(\"*\").toPandas()\n",
        "\n",
        "unique_users = _toExplore.customer_id.unique()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "customer_id  product_id       product_name  rating\n301919         91046          14   Designer Coaster       0\n282477         93825          25        White Shoes       0\n476740         45057           2    Retro surfboard       0\n628434          4003          23  Crystal Wineglass       0\n1063564        66127          26        Black Shoes       5\n528349         83350           9     Wine corkscrew       0\n1083862        31288          15   Brown Cupholders       0\n966057         64628           8      Red Corkscrew       1\n431412         20860          31         Blue Shoes       3\n1088508         1411          23  Crystal Wineglass       1"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "display(_toExplore[['customer_id', 'product_id', 'product_name', 'rating']].sample(n=10))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Training the model***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    \n",
        "    The machine learning model used is the recommendation module present in\n",
        "    pyspark.mllib.\n",
        "</p>\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Using the ALS (alternating least square) method, we can train the model, which takes a list of tuples consisting mainly of \"userID\", \"productID\" and \"rating\".\n",
        "</p>\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    The parameters passed in training the model are a list of tuples, no. of iterations, and rank.\n",
        "</p>\n",
        "<!-- <p style=\"font-size:16px; color:#117d30;\">\n",
        "    Rank is the no. of features to use while training the model.\n",
        "</p> -->\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "\n",
        "def train_model():\n",
        "  \"\"\"\n",
        "    Training model for predicting the recommendation on given set of input\n",
        "  \"\"\"\n",
        "  try:\n",
        "    rank = 5\n",
        "    numIterations = 10\n",
        "    print(\"Training model.........\")\n",
        "    \n",
        "    model = ALS.train(Customer_data, rank, numIterations, seed=30)\n",
        "    # model.save(sc, PATH)\n",
        "    return model\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return \"Error while loading model\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "Training model........."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "trained_model = train_model()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *** Loading the model***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Once the model is trained, it is saved on the required path for loading the weights generated after training the model. \n",
        "</p>\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Using the loaded model, we can generate product recommendations for customers. \n",
        "</p>\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "def load_model():\n",
        "  try:\n",
        "    saved_model = MatrixFactorizationModel.load(sc, PATH)\n",
        "    return saved_model\n",
        "  except:\n",
        "    return \"Model not loaded\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Product Recommender***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    The \"recommend_products\" method is a main wrapper function which consists of certain other methods to  recommend items to the user\". \n",
        "</p>\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "def recommend_products(user_id, num):\n",
        "  \"\"\"\n",
        "    Function for recommending products to user\n",
        "    \n",
        "    Parameters:\n",
        "      user_id    : int\n",
        "      no of product to recommend : int \n",
        "  \"\"\"\n",
        "  try:\n",
        "    user_id = int(user_id)\n",
        "      \n",
        "    check_user = validate_user(user_id)\n",
        "    \n",
        "    if len(check_user) == 0: return \"User does not exist\"\n",
        "        #       products = top_products()\n",
        "        #       return products\n",
        "        \n",
        "    data = trained_model.recommendProducts(user_id ,num)\n",
        "    result = map_products(data)\n",
        "    return result\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return \"Error while recommending product\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Validate user***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    The \"validate_user\" method is used to verify if a particualar user_id exists in the database.\n",
        "</p>\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "def validate_user(user_id):\n",
        "  \"\"\"\n",
        "    Checks if user exist in database\n",
        "    \n",
        "    Parameters:\n",
        "    \n",
        "      user_id : int\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if user_id is not None:\n",
        "      user = df.filter(df.customer_id == user_id).collect()\n",
        "      return user\n",
        "    else:\n",
        "      return \"Please pass user_id\"\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return \"Error\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Verify products***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    The \"verify_product\" method is used for checking if a product exists in the database.\n",
        "</p>"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "def verify_product(product):\n",
        "  \"\"\"\n",
        "    Validating if product exist in database\n",
        "  \"\"\"\n",
        "  try:\n",
        "    prod = df.filter(df.product_id == product).collect()\n",
        "    return prod\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return \"Error\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Map products***\n",
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    The \"map_products\" method is used to map a  product id with a product name.\n",
        "</p>"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "def map_products(data):\n",
        "  try:\n",
        "    dataFrame = pd.DataFrame(data)\n",
        "    dataFrame = dataFrame[['product', 'rating']]\n",
        "    dataFrame = pd.DataFrame(dataFrame)\n",
        "    # temp_dict = _toExplore.set_index('product_id').to_dict()['product_name']\n",
        "    # mapped_prod = dataFrame.replace(temp_dict)\n",
        "    dataFrame.rename(columns={'product':'Recommended-Products','rating':'Rating'}, inplace=True)\n",
        "    dataFrame.index.name = None\n",
        "    return dataFrame.sample(n=5)\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    return \"Error\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "new = trained_model.recommendProductsForUsers(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "userid Recommendation1  ... Recommendation4 Recommendation5\n0   18624              12  ...               5               9\n1   80704              26  ...              32              12\n2    3456              10  ...               5              31\n3    6400              12  ...               5              10\n4   24384              30  ...              10              15\n5   29696              20  ...               8              13\n6   61696              24  ...              29               4\n7   20160              24  ...              19              22\n8   59200              10  ...              16               9\n9   74816               7  ...              24               4\n10  66112              24  ...              21               4\n11  83904              10  ...              31              30\n12  33920              31  ...               3              19\n13  64960              30  ...              19              22\n14  88704              10  ...              31              30\n15   4992              12  ...               8              20\n16  79680              24  ...               4               2\n17  66368              24  ...               4              17\n18  58240              29  ...               4               6\n19  58880              12  ...              32              26\n20  62912              10  ...              12              30\n21   4224              18  ...              15              29\n22  75456              16  ...              27              22\n23  59456              24  ...              21              27\n24  64320              31  ...               3              10\n\n[25 rows x 6 columns]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "data=new.collect()\n",
        "allproduct=[]\n",
        "alluser=[]\n",
        "for  h in data:\n",
        "    t=str(h)\n",
        "    res=re.split('[\\W]+', t)\n",
        "    userid=[]\n",
        "    product=[]\n",
        "    for w in range(0,len(res)):\n",
        "        if res[w]=='user':\n",
        "            userid.append(res[w+1])\n",
        "        elif res[w]=='product':\n",
        "            product.append(res[w+1])\n",
        "    allproduct.append(product)\n",
        "    alluser.append(userid[0])\n",
        "recomm_df1=pd.DataFrame(alluser,columns=['userid'])\n",
        "recomm_df2=pd.DataFrame(allproduct,columns=['Recommendation1','Recommendation2','Recommendation3','Recommendation4','Recommendation5'])\n",
        "\n",
        "FinalData=pd.concat([recomm_df1,recomm_df2],sort=True,axis=1)\n",
        "# print(FinalData)\n",
        "FinalData.head(n=25)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size:16px; color:#117d30;\">\n",
        "    Finally, call the main function and pass the two parameters \"user_id\" and \"product_id\" to generate product recommendations.\n",
        "</p>\n",
        ""
      ],
      "attachments": {}
    }
  ]
}