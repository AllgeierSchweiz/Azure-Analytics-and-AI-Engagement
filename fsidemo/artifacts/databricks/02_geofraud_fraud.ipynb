{"cells":[{"cell_type":"markdown","source":["**DISCLAIMER**\n\nBy accessing this code, you acknowledge the code is made available for presentation and demonstration purposes only and that the code (1) is not subject to SOC 1 and SOC 2 compliance audits, and (2) is not designed or intended to be a substitute for the professional advice, diagnosis, treatment, or judgment of a certified financial services professional. Do not use this code to replace, substitute, or provide professional financial advice, or judgement. You are solely responsible for ensuring the regulatory, legal, and/or contractual compliance of any use of the code, including obtaining any authorizations or consents, and any solution you choose to build that incorporates this code in whole or in part."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"573d18bd-4719-41b6-9504-882ddc7cde4b"}}},{"cell_type":"markdown","source":["<img src=https://brysmiwasb.blob.core.windows.net/demos/geoscan/databricks_fsi_white.png width=\"600px\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ce1b22b-f1f3-4c50-ae96-faf3298a1f75"}}},{"cell_type":"markdown","source":["# Geospatial fraud detection\n\n*A large scale fraud prevention system is usually a complex ecosystem made of various controls (all with critical SLAs), a mix of traditional rules and AI and a patchwork of technologies between proprietary on-premises systems and open source cloud technologies. In a previous [solution accelerator](https://databricks.com/blog/2021/01/19/combining-rules-based-and-ai-models-to-combat-financial-fraud.html), we addressed the problem of blending rules with AI in a common orchestration layer powered by MLFlow. In this series of notebooks centered around geospatial analytics, we demonstrate how Lakehouse enables organizations to better understand customers behaviours, no longer based on who they are, but how they bank, no longer using a one-size-fits-all rule but a truly personalized AI. After all, identifying abnormal patterns can only be made possible if one first understands what a normal behaviour is, and doing so for millions of customers becomes a challenge that requires both data and AI combined into one platform. As part of this solution, we are releasing a new open source geospatial library, [GEOSCAN](https://github.com/databrickslabs/geoscan), to detect geospatial behaviours at massive scale, track customers patterns over time and detect anomalous card transactions*\n\n---\n+ <a href=\"https://databricks.com/notebooks/geoscan/00_geofraud_context.html\">STAGE0</a>: Home page\n+ <a href=\"https://databricks.com/notebooks/geoscan/01_geofraud_clustering.html\">STAGE1</a>: Using a novel approach to geospatial clustering with H3\n+ <a href=\"https://databricks.com/notebooks/geoscan/02_geofraud_fraud.html\">STAGE2</a>: Detecting anomalous transactions as ML enriched rules\n---\n<antoine.amend@databricks.com>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc3b8c74-f097-4df8-9c4a-a2e55056459f"}}},{"cell_type":"markdown","source":["## Context\nIn the previous [notebook](https://databricks.com/notebooks/geoscan/01_geofraud_clustering.html), we demonstrated how GEOSCAN can help financial services institutions leverage their entire dataset to better understand customers specific behaviours. In this notebook, we want to use the insights we have gained earlier to extract anomalous events and bridge the technological gap that exists between analytics and operations environments. More often than not, Fraud detection frameworks run outside of an analytics environment due to the combination of data sensitivity (PII), regulatory requirements (PCI/DSS) and model materiality (high SLAs and low latency). For these reasons, we explore here multiple strategies to serve our insights either as a self contained framework or through an online datastore (such as [redis](https://redis.io/), [mongodb](https://www.mongodb.com/) or [elasticache](https://aws.amazon.com/elasticache/) - although many other solutions may be viable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffaf203e-9566-4b2f-800c-afbe378ef534"}}},{"cell_type":"code","source":["%pip install pybloomfiltermmap3==0.5.3 h3==3.7.1 folium==0.12.1 mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"769edb3b-ffe1-4efd-b4fd-1dab12eec800"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Attaching transactional context to geo clusters\nAs we've trained personalized models for each customer, we can easily understand the type of transactions as well as the day and hours these transactions usually take place. Are these clusters more \"active\" during working hours or on week ends? Are these transactions more about fast foods and coffee shops or are they driving fewer but more expensives items? Such a geospatial analytics framework combined with transaction enrichment (future solution accelerator) could tell us great information about our customers' spends beyond demographics, moving towards a customer centric approach to retail banking. Unfortunately, our synthetic dataset does not contain any additional attributes to learn behavioral pattern from. For the purpose of this exercise, we will retrieve our clusters (as tiled with H3 polygon as introduced earlier) as-is to detect transactions that happened outside of any known location."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d1bc1a2-4ee6-412a-afa4-ef79063e5f8a"}}},{"cell_type":"code","source":["tiles = spark.read.table('geospatial.tiles')\ndisplay(tiles)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f39d7c3d-4c5a-4f9b-9f42-5b5fd00a6acd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As the core of our framework relies on open data standards ([RFC7946](https://tools.ietf.org/html/rfc7946)), we could load our models as a simple Dataframe without relying on the GEOSCAN library. We simply have to read the `data` directory of our model output."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e07df4a9-5bb7-4d03-8ffa-c32d89e5ec08"}}},{"cell_type":"code","source":["model_personalized = spark.read.format('parquet').load('/FileStore/demo-fsi/models/geoscan_personalized/data')\ndisplay(model_personalized)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe2ce606-8865-4c4c-b112-0f077753eb04"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Extracting anomalies\nOur (simplisitic) approach will be to detect if a transaction was executed in a popular area for each of our customers. Since we have stored and indexed all of our models as H3 tiles, it becomes easy to enrich each transaction with their cluster using a simple JOIN operation (for large scale processing) or lookup (for real time scoring) instead of complex geospatial queries like point in polygon search. Although we are using the H3 python API instead of GEOSCAN library, our generated H3 hexadecimal values are consistent - assuming we select the same resolution we used to generate those tiles (10). For reference, please have a look at the H3 [resolution table](https://h3geo.org/docs/core-library/restable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17847f22-8a0a-496c-94f1-4938ed82c75f"}}},{"cell_type":"code","source":["import h3\nfrom pyspark.sql.functions import udf\n\n@udf(\"string\")\ndef to_h3(lat, lng, precision):\n  h = h3.geo_to_h3(lat, lng, precision)\n  return h.upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1fb618e-841a-4e45-a425-59681e230df2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the example below, we can easily extract  transactions happenning outside of any customer prefered locations. Please note that we previously relaxed our conditions by adding 3 extra layers of H3 polygons to capture transactions happenning in close vicinity of spending clusters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b86a3560-bd22-4951-b9b5-211cd8721547"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nanomalous_transactions = (\n  spark\n    .read\n    .table('geospatial.transactions')\n    .withColumn('h3', to_h3(F.col('latitude'), F.col('longitude'), F.lit(10)))\n    .join(tiles, ['user', 'h3'], 'left_outer')\n    .filter(F.expr('cluster IS NULL'))\n    .drop('h3', 'cluster', 'tf_idf')\n)\n\ndisplay(anomalous_transactions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8e5b212-686b-4dc4-b186-25ba222c81ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Out of half a million transactions, we extracted 81 records in less than 5 seconds. Not necessarily fraudulent, maybe not even suspicious, these transactions did not match any of our users \"normal\" behaviours, and as such, are worth flagging as part of an overhatching fraud prevention framework. In real life example, we should factor for time and additional transactional context. Would a same transaction happening on a Sunday afternoon or a Wednesday morning be suspicious given user characteristics we could learn?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0d79ba7-4865-4071-aa97-98f81b5fcdf4"}}},{"cell_type":"markdown","source":["Before moving forwards, it is always benefitial to validate our strategy (altough not empirically) using a simple visualization for a given customer (`99407ef8-40ae-424e-b9ae-9fd2e4838ec3`), reporting card transactions happenning outside of any known patterns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9b8b3a1-a500-4e11-8e6a-d55823ad389b"}}},{"cell_type":"code","source":["import folium\nfrom folium import plugins\nfrom pyspark.sql import functions as F\n\nuser = '794ed8d5-e0f9-4454-af21-82f2a30bf3d6'\nanomalies = anomalous_transactions.filter(F.col('user') == user).toPandas()\nclusters = model_personalized.filter(F.col('user') == user).toPandas().cluster.iloc[0]\n\npersonalized = folium.Map([40.75466940037548,-73.98365020751953], zoom_start=12, width='80%', height='100%')\nfolium.TileLayer('Stamen Toner').add_to(personalized)\n\nfor i, point in anomalies.iterrows():\n  folium.Marker([point.latitude, point.longitude], popup=point.amount).add_to(personalized)\n\nfolium.GeoJson(clusters, name=\"geojson\").add_to(personalized)\npersonalized"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31985c3e-58ff-40f2-9522-bf122c9ce9a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#migrating anomalies data only  for a5359bd7-063d-489d-b247-c92ae389c7f2 as cluster data is already exported\nanomalies.to_csv('/dbfs/FileStore/geospatial_fraud_detection/anomalies_a5359bd7-063d-489d-b247-c92ae389c7f2.csv', index=False)\n#anomalies"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0375a7-eeae-4ac0-aec0-ea3a7bb9c003"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["![folium](https://brysmiwasb.blob.core.windows.net/demos/geoscan/geoscan_folium_6.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19a0d7bc-b4a3-402f-b9dc-181838cef803"}}},{"cell_type":"markdown","source":["Although this synthetic data does not show evidence of suspicious transactions, we demonstrated how anomalous records can easily be extracted from a massive dataset without the need to run complex geospatial queries. In fact, the same can now be achieved using standard SQL functionalities in a notebook or in a SQL analytics workspace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7113f61f-39fb-4f0a-bc6e-92f928a32c1d"}}},{"cell_type":"markdown","source":["## Real time fraud detection\nWith millions of transactions and low latency requirements, it would not be realistic to join datasets in real time. Although we could load all clusters (their H3 tiles) in memory, we may have evaluated multiple models at different time of the days for different users, for different segments or different transaction indicators (e.g. for different brand category or [MCC codes](https://en.wikipedia.org/wiki/Merchant_category_code)) resulting in a complex system that requires efficient lookup strategies against multiple variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e6504e4-f27a-4fda-949a-bb23df2a85ff"}}},{"cell_type":"markdown","source":["### Bloom filters\nHere comes [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter), an efficient probabilistic data structure than can test the existence of a given record without keeping an entire set in memory. Although bloom filters have been around for a long time, its usage has not - sadly - been democratized beyond complex engineering techniques such as database optimization engines and daunting execution planners (Delta engine leverages bloom filters optimizations among other techniques). This technique is a powerful tool worth having in a modern data science toolkit."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15d8af4e-8690-4a6f-9b81-2999498cf975"}}},{"cell_type":"markdown","source":["#### The theory\nThe concept behind a bloom filter is to convert a series of records (in our case a H3 location) into a series of hash values, overlaying each of their byte arrays representations as vectors of 1 and 0. Testing the existence of a given record results in testing the existence of each of its bits set to 1. Given a record `w`, if any of its bit is not found in our set, we are 100% sure we haven't seen record `w` before. However, it all of its bits are found in our set, it could be caused by an unfortunate succession of hash collisions. In other words, Bloom filters offer a false negative rate of 0 but a non zero false positive rate (records we wrongly assume have been seen) that can controlled by the length of our array and the number of hash functions.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Bloom_filter.svg/720px-Bloom_filter.svg.png\">\n\n[Source](https://en.wikipedia.org/wiki/Bloom_filter)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b53841f-5866-4ff8-b576-6fc970ddce3f"}}},{"cell_type":"markdown","source":["#### The practice\nWe will be using the `pybloomfilter` python library to validate this approach, training a Bloom filter against each and every known H3 tile of a given user. Although our filter may logically contains millions of records, we would only need to physically maintain 1 byte array in memory to enable a probabilistic search (controlled here with a 1% false positive rate)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"853bb340-7b20-4edd-acf0-fa35b411a249"}}},{"cell_type":"code","source":["import pybloomfilter\n\ndef train_bloom(records):\n  cluster = pybloomfilter.BloomFilter(len(records), 0.01)\n  cluster.update(records)\n  return cluster\n  \nrecords = list(tiles.filter(F.col('user') == user).toPandas().h3)\ncluster = train_bloom(records)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8501c713-2d73-4a91-bcb6-b1609b816ca1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We retrieve all the points we know exist and test our false negative rate (should be null)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81754ef4-6c53-4dd5-8c99-e1be60249256"}}},{"cell_type":"code","source":["normal_df = tiles.filter(F.col('user') == user).select(F.col('h3')).toPandas()\nnormal_df['matched'] = normal_df['h3'].apply(lambda x: x in cluster)\ndisplay(normal_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"add5c33a-90d7-4449-9c7c-90c1e129a6fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Similarly, we access our anomalous transactions to validate our false positive rate (should be lower than 1%)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e811e120-661b-4917-ae9d-de9101b339ee"}}},{"cell_type":"code","source":["display(anomalous_transactions.filter(F.col('user') == user))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e58e503-b62d-4774-9bb6-3b2cfe6fd6a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["abnormal_df = (\n  anomalous_transactions\n    .filter(F.col('user') == user)\n    .withColumn('h3', to_h3(F.col('latitude'), F.col('longitude'), F.lit(10)))\n    .select('h3').toPandas()\n)\nabnormal_df['matched'] = abnormal_df['h3'].apply(lambda x: x in cluster)\ndisplay(abnormal_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7873081-8477-4164-b5a4-13593fb2f692"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Training filters\nWith our approach validated, we could build one bloom filter for each user, storing an \"model\" as a simple dictionary of users <> byte array"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09cd51d5-b8e6-4683-96bf-f520d6baa1f3"}}},{"cell_type":"code","source":["user_df = tiles.groupBy('user').agg(F.collect_list('h3').alias('tiles')).toPandas()\nuser_clusters = {}\n\nfor i, rec in user_df.iterrows():\n  bloom = train_bloom(list(rec.tiles))\n  user_clusters[rec.user] = bloom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b4ab95b-7509-4b3c-9d82-5a69e67e5d54"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We now have an efficient datastructure that can be used for real time lookup without having to maintain millions of H3 tiles in memory. For a given a transaction, we convert `latitude` and `longitude` to a H3 polygon (of size 10) and query the bloom filter for that specific user. Does that card transaction happenned in a familiar place?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c419180f-2990-49e5-bdfd-0b2f26ae84a0"}}},{"cell_type":"code","source":["'8A2A100D2AD7FFF' in user_clusters[user]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab123c0e-0787-49df-9916-a1ebbcaebea6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["'8A2A100D2AD7FFF' in user_clusters[user]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"049f5ff2-4e62-430d-a8f8-c03d60c3756e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Using a `mlflow.pyFunc` pattern, we can wrap our business logic as a self packaged module that can be served real time, on stream, on SQL, or on demand, just like any ML / AI project. We just have to persist our data to disk to pass it onto our model and load bloom filters at model startup."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b29e0b-23c4-4a1b-9109-ae45ee45dc2e"}}},{"cell_type":"code","source":["_ = (\n  tiles\n    .groupBy('user')\n    .agg(F.collect_list('h3').alias('tiles'))\n    .toPandas()\n    .to_csv('/tmp/tiles')\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72402a8d-f817-4818-8b8a-492fc58fcf8f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our logic expects a dataframe of `user`, `latitude` or `longitude` as an input, appending our records with `0` or `1` (whether we have observed this location before or not)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dbd0cf4-7d47-41f9-8032-c3a4bed0703b"}}},{"cell_type":"code","source":["import mlflow\n\nclass H3Lookup(mlflow.pyfunc.PythonModel):\n    \n  def load_context(self, context): \n    import pandas as pd\n    import pybloomfilter    \n    blooms = {}\n    tiles = pd.read_csv(context.artifacts['tiles'])\n    for i, rec in user_df.iterrows():\n      records = list(rec.tiles)\n      bloom = pybloomfilter.BloomFilter(len(records), 0.1)\n      bloom.update(records)\n      blooms[rec.user] = bloom\n    self.blooms = blooms\n  \n  def predict(self, context, df):\n  \n    import h3\n    def to_h3(x):\n      h = h3.geo_to_h3(x[0], x[1], 10)\n      return h.upper()\n    \n    def is_anomalous(x):\n      if x[1] in self.blooms[x[0]]:\n        return 0\n      else:\n        return 1\n    \n    df['h3'] = df[['latitude', 'longitude']].apply(to_h3, axis=1)\n    df['anomaly'] = df[['user', 'h3']].apply(is_anomalous, axis=1)\n    return df.drop(['h3'], axis=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5460ce68-43c1-4ea8-9b1f-19d82d7d1be6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We ensure our dependencies (`pybloomfiltermmap3` and `h3`) are added to MLFlow conda environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91e72d92-ea89-4d73-a907-ffd619d15bc3"}}},{"cell_type":"code","source":["with mlflow.start_run(run_name='h3_lookup'):\n\n  conda_env = mlflow.pyfunc.get_default_conda_env()\n  conda_env['dependencies'][2]['pip'] += ['pybloomfiltermmap3==0.5.3']\n  conda_env['dependencies'][2]['pip'] += ['h3==3.7.1']\n  \n  artifacts = {\n    'tiles': '/tmp/tiles',\n  }\n  \n  mlflow.pyfunc.log_model(\n    'pipeline', \n    python_model=H3Lookup(), \n    conda_env=conda_env,\n    artifacts=artifacts\n    )\n  \n  api_run_id = mlflow.active_run().info.run_id\n  print(api_run_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ca2911f-732e-477b-a945-c1d567921189"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Model inference\nWith our model available as a `mlflow.pyfunc`, we can serve it from many different places, as a batch or on a stream, behind a custom API or a proprietary system, back on premises or using cloud based technologies. For more information about MLFlow deployment, please refer to documentation [here](https://www.mlflow.org/docs/latest/python_api/index.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5b8fadf-adaf-48e2-bff2-02897cea75a2"}}},{"cell_type":"code","source":["import mlflow\nmodel = mlflow.pyfunc.load_model('runs:/{}/pipeline'.format(api_run_id))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f7aa4a8-34fc-4530-a6b1-cf1959f1225f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transactions = spark.read.table('geospatial.transactions').toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c85da54-d514-4c51-b9d6-6e7c5fae5cf6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Out of the 133 transactions previously reported as suspicious, our bloom filters detected 78 of them (within our false positive rate) with a model that can be now executed real time in a third party environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a344918-20a6-4359-a5b5-01c139cfe9cc"}}},{"cell_type":"code","source":["anomalies = model.predict(transactions)\nanomalies = anomalies[anomalies['anomaly'] != 0]\ndisplay(anomalies)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28f2f64d-0427-4893-b8f7-955920daf24a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["However, this approach pauses an important operating challenge for large financial services organizations as new models would need to be constantly retrained and redeployed to adapt to users changing behaviours. Let's take an example a user going on holidays. Although their first card transactions may be returned as anomalous (not necessarily suspicious), such a strategy would need to adapt and learn the new \"normal\" as more and more transactions are observed. One would need to run the same process with new data, resulting in a new version of a model being released, reviewed by an independant team of experts, approved by a governance entity and eventually updated to a fraud production endpoints outside of any change freeze. Technically possible when supported by a strong operating model (data driven organizations), this approach may not be viable for many."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6cd1d47-7c5d-45ae-9bcb-c2f76702a148"}}},{"cell_type":"markdown","source":["### Online datastore\nIt is fairly common for financial services institutions to have an online data store decoupled from an analytics platform. A flow of incoming transactions are compared with reference data points in real time. An alternative approach to the above is to use an online datastore (like mongodb) to keep \"pushing\" reference data points to a live endpoint as a business as usual process (hence outside of ITSM change windows). Any incoming transaction would be matched against a set of rules constantly updated (reference data) and accessible via sub-seconds look up queries. Using [mongo db connector](https://docs.mongodb.com/spark-connector/current/) (as an example), we show how organizations can save our geo clusters dataframes for real time serving, combining the predictive power of advanced analytics with low latency of traditional rule based systems."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7653c274-27dc-462b-8c31-9293d44e0476"}}},{"cell_type":"markdown","source":["#### Time to live\nLet's first create a new collection (i.e. a table) on mongodb and create an index with a [Time to Live](https://docs.mongodb.com/manual/tutorial/expire-data/) parameter (TTL). Besides the operation benefits not having to maintain this collection (records are purged after TTL expires) we can bring a temporal dimension to our model in order to cope with users changing patterns. With a TTL of e.g. 1 week and a new model trained every day, we can track clusters over time and dynamically adapt our fraud strategy as new transactions are being observed whilst keeping track of historical records\n\n```\n>> mongo\n>> use fraud\n>> db.tiles.createIndex( { \"createdAt\": 1 }, { expireAfterSeconds: 604800 } )\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a06b7ee-9e4e-4f23-9c45-478dc4f031d3"}}},{"cell_type":"code","source":["%scala\nval tiles = spark.read.table(\"geospatial.tiles\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb7dd612-5fdc-4b96-b424-9ffa5b3a5556"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// import com.mongodb.spark._\n// import org.apache.spark.sql.functions._\n\n// tiles\n//   .withColumn(\"createdAt\", current_timestamp())  \n//   .write\n//   .format(\"mongo\")\n//   .mode(\"append\")\n//   .option(\"database\", \"fraud\")\n//   .option(\"collection\", \"tiles\")\n//   .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f0461c9-a380-43e2-831a-ac5fc182a0f9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["An online operation process could be monitoring new card transactions in real time by simply searching for specific H3 tiles of a given user via a simple mongo db search query\n\n```\nuse fraud\ndb.tiles.find({\"user\": \"7f103b53-25b4-483d-81f2-e646d22930b2\", \"tile\": \"8A2A1008916FFFF\"})\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"256964ff-23db-4a27-bf6c-57ab472ea1d6"}}},{"cell_type":"markdown","source":["In the visualization below, we show an example of how change in customers' transactional behaviours could be tracked over time (thanks to our TTL), where any observed location stays active for a period of X days and wherein anomalous transactions can be detected in real time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85ae4a15-d400-4b0a-a507-a802e700d70c"}}},{"cell_type":"markdown","source":["![window](https://brysmiwasb.blob.core.windows.net/demos/geoscan/geoscan_window.gif)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10d238d2-9177-43c3-9493-247dac7cb64d"}}},{"cell_type":"markdown","source":["## Closing thoughts\n\nCard fraud transactions will never be addressed by a one-size-fits-all model but should always make use of various indicators coming from different controls as part of an overhatching fraud prevention strategy. Often, this combines [AI models with a rule based systems](https://databricks.com/blog/2021/01/19/combining-rules-based-and-ai-models-to-combat-financial-fraud.html), integrates advanced technologies and legacy processes, cloud based infrastructures and on premises systems, and must comply with tight regulatory requirements and critical SLAs. Although our approach does not aim at identifying fraudulent transactions on its own, it strongly contributes at extracting anomalous events in an **timely**, **cost effective** (self maintained) and fully **explainable** manner, hence a great candidate to combat financial fraud more effectively in a coordinated rules + AI strategy.\n\nAs part of this exercise, we also discovered something equally important in financial services. We demonstrated the ability of a Lakehouse infrastructure to transition from traditional to personalized banking where consumers are no longer segmented by demographics (who they are) but by their spending patterns (how they bank), paving the way towards a more customer centric future of retail banking."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fcdd5a0-4764-41b8-9c7a-aa20eb5a65ad"}}},{"cell_type":"markdown","source":["---\n+ <a href=\"https://databricks.com/notebooks/geoscan/00_geofraud_context.html\">STAGE0</a>: Home page\n+ <a href=\"https://databricks.com/notebooks/geoscan/01_geofraud_clustering.html\">STAGE1</a>: Using a novel approach to geospatial clustering with H3\n+ <a href=\"https://databricks.com/notebooks/geoscan/02_geofraud_fraud.html\">STAGE2</a>: Detecting anomalous transactions as ML enriched rules\n---"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd8bcbef-0e97-4697-b033-e54e63c49f2a"}}},{"cell_type":"markdown","source":["&copy; 2021 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source].  All included or referenced third party libraries are subject to the licenses set forth below.\n\n| library                                | description             | license    | source                                              |\n|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n| com.uber:h3:3.6.3                      | Uber geospatial library | Apache2    | https://github.com/uber/h3                          |\n| h3                                     | Uber geospatial library | Apache2    | https://github.com/uber/h3-py                       |\n| org.scala-graph:graph-core_2.12:1.12.5 | Scala graph             | Apache2    | https://github.com/scala-graph/scala-graph          |\n| com.databricks.labs:geoscan:0.0.1      | Geoscan algorithm       | Databricks | https://github.com/databrickslabs/geoscan           |\n| folium                                 | Geospatial visualization| MIT        | https://github.com/python-visualization/folium      |\n| pybloomfiltermmap3                     | Bloom filter            | MIT        | https://github.com/prashnts/pybloomfiltermmap3      |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4321d77-d79e-49ce-87dd-d834c3b032aa"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02_geofraud_fraud","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"experimentId":"2104198048372818"},"language":"python","widgets":{},"notebookOrigID":2104198048372818}},"nbformat":4,"nbformat_minor":0}
