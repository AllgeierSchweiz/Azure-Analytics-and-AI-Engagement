{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"14259cd7-4008-4682-b505-dd90675f8663","showTitle":false,"title":""}},"source":["#SQL Analytics On Delta Live Tables<br/>\n","\n","1. **Application           :** Social Media Analytics <br/>\n","2. **Usecase               :** Performing SQL analytics and Machine Learning algorithms on Twitter data coming incrementally from data lake.<br/>\n","3. **Notebook Summary      :** This notebook is a part of social media analytics application which perform `SQL Analytics`.<br/>\n","4. **Notebook Description  :** Performing SQL Analytics on top of Silver layer to derive `Gold layer`, which has aggregated/curated data for visuals and dashboards.\n","\n","\n","###Feature List\n","1. Lakehouse Architecture\n","2. Batch processing Twitter Messages\n","3. Delta Live Tables (Bronze, Silver and Gold and aggregation)\n","4. Citywise Hashtag Count\n","5. Z-Order Optimization \n","6. Table Caching\n","7. Time Travel"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"db9bb3e8-235b-4b45-bb60-99c56cb37298","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Python interpreter will be restarted.\n","Requirement already satisfied: geopy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (2.2.0)\n","Requirement already satisfied: folium in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (0.12.1.post1)\n","Requirement already satisfied: geographiclib<2,>=1.49 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (from geopy) (1.52)\n","Requirement already satisfied: jinja2>=2.9 in /databricks/python3/lib/python3.9/site-packages (from folium) (2.11.3)\n","Requirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from folium) (1.20.3)\n","Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from folium) (2.26.0)\n","Requirement already satisfied: branca>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (from folium) (0.5.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2>=2.9->folium) (2.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (3.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (2.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (2021.10.8)\n","Python interpreter will be restarted.\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Python interpreter will be restarted.\nRequirement already satisfied: geopy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (2.2.0)\nRequirement already satisfied: folium in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (0.12.1.post1)\nRequirement already satisfied: geographiclib<2,>=1.49 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (from geopy) (1.52)\nRequirement already satisfied: jinja2>=2.9 in /databricks/python3/lib/python3.9/site-packages (from folium) (2.11.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from folium) (1.20.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from folium) (2.26.0)\nRequirement already satisfied: branca>=0.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cecf6a2c-b0a7-4d2c-b49e-bf8570a7d9fd/lib/python3.9/site-packages (from folium) (0.5.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2>=2.9->folium) (2.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->folium) (2021.10.8)\nPython interpreter will be restarted.\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%pip install geopy folium"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fb975ae9-bc2a-4454-baac-67f00573a1f4","showTitle":false,"title":""}},"outputs":[],"source":["import requests\n","import base64\n","import mimetypes\n","\n","def url_image_to_base64_type_tuple(img_url):\n","  \"\"\"\n","  Provide a URL to an image and have it converted to base64.\n","  - Url e.g.: `https://databricks.com/wp-content/themes/databricks/assets/images/header_logo_2x.png`\n","  Returns tuple of base64 data, mime_type\n","  \"\"\"\n","  # notice the additional decode('utf-8') call\n","  response = requests.get(img_url)\n","  b64_data = str(base64.b64encode(response.content).decode('utf-8'))\n","  return b64_data, response.headers['Content-Type']\n","\n","def display_img(base64_data, mime_type, width_percent=None):\n","  \"\"\"\n","  Use `displayHTML` to display the base64 data using mime type).\n","  - If width_percent is provided, will set as attribute on `img` element.\n","  \"\"\"\n","  if width_percent is None:\n","    displayHTML(f\"\"\"<img src=\"data:{mime_type};base64,{base64_data}\">\"\"\") \n","  else:\n","    displayHTML(f\"\"\"<img src=\"data:{mime_type};base64,{base64_data}\" width=\"{width_percent}%\">\"\"\") \n","    \n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ccbce8b2-13b8-4517-9d53-952ed6909f55","showTitle":false,"title":""}},"source":["#### Raw Twitter Data - Bronze\n","Our bronze layer stores the raw, unprocessed data from our Twitter API pulls. By leaving it in its raw state, we give ourselves the option to reprocess it for different purposes in the future. Thanks to Azure Data Lake Gen 2, we can maintain this data for as long as we need it at very low costs. The bronze layer is usually the domain of *data engineers* who then build pipelines to refine this data forward into the silver layer."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"010b1709-b66a-4a07-bb14-7905fd082edc","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n","\u001b[0;32m<command-2040825476367559>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n","\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m<command-2040825476367559>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n","\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"U2VsZWN0IFRpbWUsIEhhc2h0YWcsIENpdHksIFJldHdlZXRDb3VudCwgRmF2b3VyaXRlQ291bnQsIElzUmV0d2VldCwgSG91ck9mRGF5LCBMYW5ndWFnZSAKZnJvbSBsYWtlZGIuYnJvbnplX3R3aXR0ZXJfaGlzdG9yaWNhbF9kYXRh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n","\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n","\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n","\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n","\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n","\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: lakedb.bronze_twitter_historical_data; line 2 pos 5;\n","'Project ['Time, 'Hashtag, 'City, 'RetweetCount, 'FavouriteCount, 'IsRetweet, 'HourOfDay, 'Language]\n","+- 'UnresolvedRelation [lakedb, bronze_twitter_historical_data], [], false\n"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-2040825476367559>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_sqldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m<command-2040825476367559>\u001b[0m in \u001b[0;36m____databricks_percent_sql\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m____databricks_percent_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"U2VsZWN0IFRpbWUsIEhhc2h0YWcsIENpdHksIFJldHdlZXRDb3VudCwgRmF2b3VyaXRlQ291bnQsIElzUmV0d2VldCwgSG91ck9mRGF5LCBMYW5ndWFnZSAKZnJvbSBsYWtlZGIuYnJvbnplX3R3aXR0ZXJfaGlzdG9yaWNhbF9kYXRh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 logger.log_success(\n\u001b[1;32m     50\u001b[0m                     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: lakedb.bronze_twitter_historical_data; line 2 pos 5;\n'Project ['Time, 'Hashtag, 'City, 'RetweetCount, 'FavouriteCount, 'IsRetweet, 'HourOfDay, 'Language]\n+- 'UnresolvedRelation [lakedb, bronze_twitter_historical_data], [], false\n","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table or view not found: lakedb.bronze_twitter_historical_data; line 2 pos 5;\n'Project ['Time, 'Hashtag, 'City, 'RetweetCount, 'FavouriteCount, 'IsRetweet, 'HourOfDay, 'Language]\n+- 'UnresolvedRelation [lakedb, bronze_twitter_historical_data], [], false\n","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","Select Time, Hashtag, City, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language \n","from lakedb.bronze_twitter_historical_data\n","\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9d1dba98-7f3a-4669-8b04-2623b3b164b8","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","Select Time, Hashtag, City, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language  \n","from lakedb.bronze_twitter_historical_data\n","Where City <> 'Online'"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"53b86b3b-c8a7-4b73-a8cf-cd4375815509","showTitle":false,"title":""}},"source":["### Filtered Twitter Data - Silver\n","In our silver layer, we've curated our raw Twitter data into something more usable for *data scientists.* They can take these cleaned up, detailed level tables and develop features for machine learning models as well as aggregated analytical datasets for data analysts."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"12116290-e471-40a8-abbd-eb835a038cd7","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","Select Time, Hashtag, City, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language\n","from lakedb.silver_twitter_historical_data"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2db487ae-3096-492e-aa64-da6c54cadc9b","showTitle":false,"title":""}},"source":["#### Curated Twitter Data - Gold\n","\n","In our gold layer, we can enhance and refine our silver data sets even further into fit-for-purpose tables and views for specific analytical needs. Here we've augmented our Twitter data with a machine learning model identifying the sentiment (positive, neutral, or negative) of each Tweet so we can get a sense of how the Twitter tags we're analyzing are being used."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"78ee3f67-9e91-4f9e-b830-1475c1336db4","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","Select MLSentiment,Time, Hashtag, City, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language  \n","from lakedb.gold_twitter_historical_data"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"55067378-1a5a-4793-8a6b-4e283080c3a2","showTitle":false,"title":""}},"source":["#### Aggregations\n","\n","By pre-emptively aggregating our data that rarely or slowly changes, we can provide a great performance benefit for our end users. Our DLT pipeline performs this aggregation of hashtag counts by the geolocation of the Tweets. By only updating this each time we ingest more Tweets, we can keep the aggregation table up to date and then quickly consume and visualize it in tools like Power BI.\n","\n","One nice feature of Databricks notebooks is if a cell produces a DataFrame output (like the one below), you can also profile the data as well as generate quick visualizations. Throw in Markdown and comments and notebooks are a super convenient way to collaborate and communicate with your team, leadership, customers, and other stakeholders."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bb7646f7-4cea-47f2-8527-dad73f3cd460","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","SELECT City, Hashtag, Count from lakedb.gold_twitter_historical_city_hashtagcount_data where city is not null order by count desc limit 10"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"972bcb6a-836d-4e19-bb8a-2f8a826efa6c","showTitle":false,"title":""}},"source":["One way to analyze our Twitter data is by geolocation. We can easily plot our Twitter hashtag counts from above on a fully interactive map visual - directly within our notebook! We could then share this notebook with others so they can explore the location and hashtag data in more detail."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"decbf7f3-abfe-4bb8-9b77-393a641757f0","showTitle":true,"title":"Define geocoordinate function to get latitude and longitude based on Twitter location"}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from geopy.geocoders import Nominatim\n","geolocator = Nominatim(user_agent='twitter-analysis-cl')\n","#from pyspark.sql.functions import udf\n","\n","def lon_lat(city):\n","  lon = geolocator.geocode(city)[1][1]\n","  lat = geolocator.geocode(city)[1][0]\n","  return lat, lon"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a675ce31-1b6d-48ce-9e83-9683f618d350","showTitle":true,"title":"Apply geocoordinate function to each city in our dataset"}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df = spark.sql(\"SELECT * from lakedb.gold_twitter_historical_city_hashtagcount_data where city is not null AND city NOT IN ('MFworld', 'KWANGYA', 'India', '#codedaily') order by count desc limit 10\")\n","df = df.toPandas()\n","df = df.fillna(\"\")\n","df['latitude'], df['longitude'] = zip(*df.apply(lambda x: lon_lat(x['city']), axis = 1)) #Looking up each city can take awhile! :)\n","display(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a6e09cea-51e1-4d47-8ed7-f0ce3b2c44d2","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["hashtagcountdf=spark.createDataFrame(df) \n","hashtagcountdf.write \\\n","  .format(\"delta\") \\\n","  .mode(\"overwrite\") \\\n","  .saveAsTable(\"lakedb.gold_twitter_historical_city_hashtag_count_loc\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1afdd8bc-a1f1-4c73-86bf-1a5d9b93ba28","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","SELECT * from lakedb.gold_twitter_historical_city_hashtag_count_loc"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"df24f52a-0880-4bbc-9c2c-d80e2224723f","showTitle":false,"title":""}},"source":["#### Create world map - Twitter Messages\n","Map created below shows the location from where people were tweeting"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"efa55dcf-15bf-4e0a-ab59-0f8a7726429a","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# Import general useful packages\n","import numpy as np\n","import pandas as pd\n","\n","# Create a world map to show distributions of users \n","import folium\n","from folium.plugins import MarkerCluster\n","\n","# Empty map\n","world_map= folium.Map(tiles=\"cartodbpositron\")\n","marker_cluster = MarkerCluster().add_to(world_map)\n","\n","# Creating map\n","for i in range(len(df)):\n","  lat = df.iloc[i]['latitude']\n","  lon = df.iloc[i]['longitude']\n","  radius=5\n","  popup_text = \"\"\"Country : {}<br>\"\"\"\n","  popup_text = popup_text.format(df.iloc[i]['city'])\n","  folium.CircleMarker(location = [lat, lon], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n","\n","#show the map\n","world_map"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b191a41f-990a-481f-90fc-00843799d049","showTitle":false,"title":""}},"source":["#### Z-Order Optimization\n","Z-Ordering is a technique to colocate related information in the same set of files. <br/> This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms. <br/> This behaviour dramatically reduces the amount of data that Delta Lake on Databricks needs to read"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bcd8aad6-fcaa-451d-9f4d-f5b383666080","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql \n","OPTIMIZE lakedb.gold_twitter_historical_data ZORDER BY city"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7463d0dc-c086-496d-9fc9-7511988056e8","showTitle":false,"title":""}},"source":["####Caching\n","Caching reduces scanning of the original files in future queries. It basically caches contents of a table in Apache Spark cache. If a query is cached, then a temp view is created for this query."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"66a71afc-8fa0-4af5-86b7-d988cc94d3b6","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql \n","CACHE SELECT * FROM lakedb.gold_twitter_historical_data"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"61133a7e-9093-451c-829e-953b97a0dbd4","showTitle":false,"title":""}},"source":["### Time Travel\n","In audit history above we can view the history of the different versions of the table and load and display any of those versions. <br/>\n","In the exercise below we have displayed data in a specific version"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"250750d9-417f-4442-9e66-fda291603501","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["from pyspark.sql.functions import *\n","history = spark.sql(\"DESCRIBE HISTORY lakedb.`gold_twitter_historical_data`\")\n","latest_version = history.selectExpr(\"max(version)\").collect()\n","version = (latest_version[0][0])\n","print(\"Version:\") \n","print(version)\n","df = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(\"dbfs:/mnt/delta-files/dlt/tables/gold_twitter_historical_data\")\n","print(\"Rows:\") \n","print(df.count())\n","display(df.select(col(\"Time\"),col(\"MLSentiment\"),col(\"Hashtag\"),col(\"City\"),col(\"RetweetCount\"),col(\"FavouriteCount\"),col(\"IsRetweet\"),col(\"HourOfDay\"),col(\"Language\")))"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2040825476367559,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"04_SQL_Analytics_On_Delta_Live_Tables.ipynb","notebookOrigID":1045206366549569,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.0"},"vscode":{"interpreter":{"hash":"3141a49723781b98b7f680f9cbca6d3e5baf829556b062ddd50622f33b8cf8aa"}}},"nbformat":4,"nbformat_minor":0}
