{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"header.jpg\" alt=\"Wide World Importers\" width=\"400\"/>\n",
        "<img src=\"iStock-1328873668.jpg\" alt=\"Wide World Importers - Solar Panel\" width=\"500\"/>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "import azureml.core\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from azureml.automl.core.featurization import FeaturizationConfig\n",
        "from azureml.core import Dataset, Experiment, Workspace\n",
        "from azureml.train.automl import AutoMLConfig"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1675341308229
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is compatible with Azure ML SDK version 1.35.0 or later."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You are currently using version 1.48.0 of the Azure ML SDK\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1675341308419
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# choose a name for the run history container in the workspace\n",
        "experiment_name = \"demand_forecast_exp\"\n",
        "\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "\n",
        "output = {}\n",
        "output[\"Subscription ID\"] = ws.subscription_id\n",
        "output[\"Workspace\"] = ws.name\n",
        "output[\"SKU\"] = ws.sku\n",
        "output[\"Resource Group\"] = ws.resource_group\n",
        "output[\"Location\"] = ws.location\n",
        "output[\"Run History Name\"] = experiment_name\n",
        "output[\"SDK Version\"] = azureml.core.VERSION\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
        "outputDf.T"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "                                                      \nSubscription ID   506e86fc-853c-4557-a6e5-ad72114efd2b\nWorkspace                                   amlws-midp\nSKU                                              Basic\nResource Group             rg-midpwithazurecosmos-prod\nLocation                                       eastus2\nRun History Name                   demand_forecast_exp\nSDK Version                                     1.48.0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Subscription ID</th>\n      <td>506e86fc-853c-4557-a6e5-ad72114efd2b</td>\n    </tr>\n    <tr>\n      <th>Workspace</th>\n      <td>amlws-midp</td>\n    </tr>\n    <tr>\n      <th>SKU</th>\n      <td>Basic</td>\n    </tr>\n    <tr>\n      <th>Resource Group</th>\n      <td>rg-midpwithazurecosmos-prod</td>\n    </tr>\n    <tr>\n      <th>Location</th>\n      <td>eastus2</td>\n    </tr>\n    <tr>\n      <th>Run History Name</th>\n      <td>demand_forecast_exp</td>\n    </tr>\n    <tr>\n      <th>SDK Version</th>\n      <td>1.48.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1675341309728
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute\n",
        "You will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) for your AutoML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
        "\n",
        "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
        "\n",
        "#### Creation of AmlCompute takes approximately 5 minutes. \n",
        "If the resource with AmlCompute name already exists in your workspace, this code will skip the creation process.\n",
        "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your cluster.\n",
        "amlcompute_cluster_name = \"wwi-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
        "    print(\"Found existing cluster, use it.\")\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(\n",
        "        vm_size=\"STANDARD_DS12_V2\", max_nodes=4\n",
        "    )\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "InProgress.\nSucceededProvisioning operation finished, operation \"Succeeded\"\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1675341316015
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace) is paired with the storage account, which contains the default data store. We will use it to upload the solar panel demand data and create [tabular dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = ws.get_default_datastore()\n",
        "datastore.upload_files(\n",
        "    files=[\"solar-panel-demand-no.csv\"], target_path=\"dataset/\", overwrite=True, show_progress=True\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_19a0bdf8638b4dd7ab381fbb993f7840"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1675341317034
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up what we know about the dataset. \n",
        "\n",
        "**Target column** is what we want to forecast.\n",
        "\n",
        "**Time column** is the time axis along which to predict."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target_column_name = \"cnt\"\n",
        "time_column_name = \"date\""
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1675341317208
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.Tabular.from_delimited_files(\n",
        "    path=[(datastore, \"dataset/solar-panel-demand-no.csv\")]\n",
        ").with_timestamp_columns(fine_grain_timestamp=time_column_name)\n",
        "\n",
        "# Drop the columns 'casual' and 'registered' as these columns are a breakdown of the total and therefore a leak.\n",
        "dataset = dataset.drop_columns(columns=[\"casual\", \"registered\"])\n",
        "\n",
        "dataset.take_sample(0.01).to_pandas_dataframe().reset_index(drop=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "   instant       date  season  yr  mnth  weekday  weathersit  incentive_level  \\\n0       23 2011-01-23       1   0     1        0           1         0.096522   \n1       28 2011-01-28       1   0     1        5           2         0.203478   \n2      208 2011-07-27       3   0     7        3           1         0.775000   \n3      230 2011-08-18       3   0     8        4           1         0.711667   \n4      440 2012-03-15       1   1     3        4           1         0.557500   \n5      693 2012-11-23       4   1    11        5           1         0.368333   \n\n   incentive_duration_remaining  power_grid_premium  power_grid_risk   cnt  \n0                      0.098839            0.436522         0.246600   986  \n1                      0.223317            0.793043         0.123300  1167  \n2                      0.690667            0.402917         0.183463  4656  \n3                      0.662258            0.654583         0.233208  3805  \n4                      0.532825            0.579583         0.149883  6192  \n5                      0.378779            0.568750         0.148021  3910  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>date</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>weekday</th>\n      <th>weathersit</th>\n      <th>incentive_level</th>\n      <th>incentive_duration_remaining</th>\n      <th>power_grid_premium</th>\n      <th>power_grid_risk</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>2011-01-23</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.096522</td>\n      <td>0.098839</td>\n      <td>0.436522</td>\n      <td>0.246600</td>\n      <td>986</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28</td>\n      <td>2011-01-28</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0.203478</td>\n      <td>0.223317</td>\n      <td>0.793043</td>\n      <td>0.123300</td>\n      <td>1167</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>208</td>\n      <td>2011-07-27</td>\n      <td>3</td>\n      <td>0</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.775000</td>\n      <td>0.690667</td>\n      <td>0.402917</td>\n      <td>0.183463</td>\n      <td>4656</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>230</td>\n      <td>2011-08-18</td>\n      <td>3</td>\n      <td>0</td>\n      <td>8</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.711667</td>\n      <td>0.662258</td>\n      <td>0.654583</td>\n      <td>0.233208</td>\n      <td>3805</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>440</td>\n      <td>2012-03-15</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.557500</td>\n      <td>0.532825</td>\n      <td>0.579583</td>\n      <td>0.149883</td>\n      <td>6192</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>693</td>\n      <td>2012-11-23</td>\n      <td>4</td>\n      <td>1</td>\n      <td>11</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.368333</td>\n      <td>0.378779</td>\n      <td>0.568750</td>\n      <td>0.148021</td>\n      <td>3910</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1675341326848
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the data\n",
        "\n",
        "We first split the dataset into train and test sets. Note that we are splitting on the basis of time. Data before 9/1 will be used for training, and data after and including 9/1 will be used for testing."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# select data that occurs before a specified date\n",
        "train = dataset.time_before(datetime(2012, 8, 31), include_boundary=True)\n",
        "train.to_pandas_dataframe().sample(5).reset_index(drop=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "   instant       date  season  yr  mnth  weekday  weathersit  incentive_level  \\\n0      321 2011-11-17       4   0    11        4           2         0.341667   \n1      331 2011-11-27       4   0    11        0           1         0.459167   \n2      354 2011-12-20       4   0    12        2           2         0.385833   \n3       81 2011-03-22       2   0     3        2           1         0.441667   \n4       37 2011-02-06       1   0     2        0           1         0.285833   \n\n   incentive_duration_remaining  power_grid_premium  power_grid_risk   cnt  \n0                      0.323221            0.575833         0.305362  3053  \n1                      0.455800            0.698333         0.208954  3071  \n2                      0.396454            0.595417         0.061571  3750  \n3                      0.440642            0.624583         0.225750  2703  \n4                      0.291671            0.568333         0.141800  1623  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>date</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>weekday</th>\n      <th>weathersit</th>\n      <th>incentive_level</th>\n      <th>incentive_duration_remaining</th>\n      <th>power_grid_premium</th>\n      <th>power_grid_risk</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>321</td>\n      <td>2011-11-17</td>\n      <td>4</td>\n      <td>0</td>\n      <td>11</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.341667</td>\n      <td>0.323221</td>\n      <td>0.575833</td>\n      <td>0.305362</td>\n      <td>3053</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>331</td>\n      <td>2011-11-27</td>\n      <td>4</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.459167</td>\n      <td>0.455800</td>\n      <td>0.698333</td>\n      <td>0.208954</td>\n      <td>3071</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>354</td>\n      <td>2011-12-20</td>\n      <td>4</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.385833</td>\n      <td>0.396454</td>\n      <td>0.595417</td>\n      <td>0.061571</td>\n      <td>3750</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>81</td>\n      <td>2011-03-22</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.441667</td>\n      <td>0.440642</td>\n      <td>0.624583</td>\n      <td>0.225750</td>\n      <td>2703</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37</td>\n      <td>2011-02-06</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.285833</td>\n      <td>0.291671</td>\n      <td>0.568333</td>\n      <td>0.141800</td>\n      <td>1623</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1675341327705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = dataset.time_after(datetime(2012, 9, 1), include_boundary=True)\n",
        "test.to_pandas_dataframe().sample(5).reset_index(drop=True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "   instant       date  season  yr  mnth  weekday  weathersit  incentive_level  \\\n0      624 2012-09-15       3   1     9        6           1         0.608333   \n1      643 2012-10-04       4   1    10        4           2         0.657500   \n2      652 2012-10-13       4   1    10        6           1         0.393333   \n3      630 2012-09-21       3   1     9        5           1         0.599167   \n4      664 2012-10-25       4   1    10        4           2         0.550000   \n\n   incentive_duration_remaining  power_grid_premium  power_grid_risk   cnt  \n0                      0.585867            0.501667         0.247521  8714  \n1                      0.607975            0.722917         0.117546  7328  \n2                      0.391396            0.494583         0.146142  7109  \n3                      0.571971            0.668750         0.154229  8167  \n4                      0.529688            0.800417         0.124375  7359  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>date</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>weekday</th>\n      <th>weathersit</th>\n      <th>incentive_level</th>\n      <th>incentive_duration_remaining</th>\n      <th>power_grid_premium</th>\n      <th>power_grid_risk</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>624</td>\n      <td>2012-09-15</td>\n      <td>3</td>\n      <td>1</td>\n      <td>9</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.608333</td>\n      <td>0.585867</td>\n      <td>0.501667</td>\n      <td>0.247521</td>\n      <td>8714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>643</td>\n      <td>2012-10-04</td>\n      <td>4</td>\n      <td>1</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.657500</td>\n      <td>0.607975</td>\n      <td>0.722917</td>\n      <td>0.117546</td>\n      <td>7328</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>652</td>\n      <td>2012-10-13</td>\n      <td>4</td>\n      <td>1</td>\n      <td>10</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.393333</td>\n      <td>0.391396</td>\n      <td>0.494583</td>\n      <td>0.146142</td>\n      <td>7109</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>630</td>\n      <td>2012-09-21</td>\n      <td>3</td>\n      <td>1</td>\n      <td>9</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0.599167</td>\n      <td>0.571971</td>\n      <td>0.668750</td>\n      <td>0.154229</td>\n      <td>8167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>664</td>\n      <td>2012-10-25</td>\n      <td>4</td>\n      <td>1</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.550000</td>\n      <td>0.529688</td>\n      <td>0.800417</td>\n      <td>0.124375</td>\n      <td>7359</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1675341328028
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting Parameters\n",
        "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below describes the forecasting parameter we will pass into our experiment.\n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**time_column_name**|The name of your time column.|\n",
        "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the time series frequency (e.g. daily, weekly).|\n",
        "|**country_or_region_for_holidays**|The country/region used to generate holiday features. These should be ISO 3166 two-letter country/region codes (i.e. 'US', 'GB').|\n",
        "|**target_lags**|The target_lags specifies how far back we will construct the lags of the target variable.|\n",
        "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
        "|**cv_step_size**|Number of periods between two consecutive cross-validation folds. The default value is \"auto\", in which case AutoML determines the cross-validation step size automatically, if a validation set is not provided. Or users could specify an integer value."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**task**|forecasting|\n",
        "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
        "|**blocked_models**|Models in blocked_models won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
        "|**experiment_timeout_hours**|Experimentation timeout in hours.|\n",
        "|**training_data**|Input dataset, containing both features and label column.|\n",
        "|**label_column_name**|The name of the label column.|\n",
        "|**compute_target**|The remote compute for training.|\n",
        "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection. The default value is \"auto\", in which case AutoMl determines the number of cross-validations automatically, if a validation set is not provided. Or users could specify an integer value.\n",
        "|**enable_early_stopping**|If early stopping is on, training will stop when the primary metric is no longer improving.|\n",
        "|**forecasting_parameters**|A class that holds all the forecasting related parameters.|\n",
        "\n",
        "This notebook uses the blocked_models parameter to exclude some models that take a longer time to train on this dataset. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting forecaster maximum horizon \n",
        "\n",
        "The forecast horizon is the number of periods into the future that the model should predict. Here, we set the horizon to 14 periods (i.e. 14 days). Notice that this is much shorter than the number of days in the test set; we will need to use a rolling test to evaluate the performance on the whole test set. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_horizon = 14"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1675341328302
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert prediction type to integer\n",
        "The featurization configuration can be used to change the default prediction type from decimal numbers to integer. This customization can be used in the scenario when the target column is expected to contain whole values as the number of Solar Panels sold per day."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "featurization_config = FeaturizationConfig()\n",
        "# Force the target column, to be integer type.\n",
        "featurization_config.add_prediction_transform_type(\"Integer\")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1675341328550
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config AutoML"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
        "\n",
        "forecasting_parameters = ForecastingParameters(\n",
        "    time_column_name=time_column_name,\n",
        "    freq=\"D\",  # Set the forecast frequency to be daily\n",
        "    cv_step_size=\"auto\",\n",
        ")\n",
        "\n",
        "automl_config = AutoMLConfig(\n",
        "    task=\"forecasting\",\n",
        "    primary_metric=\"normalized_root_mean_squared_error\",\n",
        "    featurization=featurization_config,\n",
        "    blocked_models=[\"ExtremeRandomTrees\"],\n",
        "    experiment_timeout_hours=0.3,\n",
        "    training_data=train,\n",
        "    label_column_name=target_column_name,\n",
        "    compute_target=compute_target,\n",
        "    enable_early_stopping=True,\n",
        "    n_cross_validations=\"auto\",  # Feel free to set to a small integer (>=2) if runtime is an issue.\n",
        "    max_concurrent_iterations=4,\n",
        "    max_cores_per_iteration=-1,\n",
        "    verbosity=logging.INFO,\n",
        "    forecasting_parameters=forecasting_parameters,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1675341328769
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now run the experiment, you can go to Azure ML portal to view the run details. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run = experiment.submit(automl_config, show_output=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Submitting remote run.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>demand_forecast_exp</td><td>AutoML_4ff0c6b7-02d8-400c-936f-625078ced73c</td><td>automl</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/AutoML_4ff0c6b7-02d8-400c-936f-625078ced73c?wsid=/subscriptions/506e86fc-853c-4557-a6e5-ad72114efd2b/resourcegroups/rg-midpwithazurecosmos-prod/workspaces/amlws-midp&amp;tid=f94768c8-8714-4abe-8e2d-37a64b18216a\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1675341331563
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1675272279499
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve the Best Run details\n",
        "Below we retrieve the best Run object from among all the runs in the experiment."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = remote_run.get_best_child()\n",
        "best_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272279922
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Featurization\n",
        "\n",
        "We can look at the engineered feature names generated in time-series featurization via the JSON file named 'engineered_feature_names.json' under the run outputs. Note that a number of named holiday periods are represented. We recommend that you have at least one year of data when using this feature to ensure that all yearly holidays are captured in the training featurization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the JSON file locally\n",
        "best_run.download_file(\n",
        "    \"outputs/engineered_feature_names.json\", \"engineered_feature_names.json\"\n",
        ")\n",
        "with open(\"engineered_feature_names.json\", \"r\") as f:\n",
        "    records = json.load(f)\n",
        "\n",
        "records"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272283611
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View the featurization summary\n",
        "\n",
        "You can also see what featurization steps were performed on different raw features in the user data. For each raw feature in the user data, the following information is displayed:\n",
        "\n",
        "- Raw feature name\n",
        "- Number of engineered features formed out of this raw feature\n",
        "- Type detected\n",
        "- If feature was dropped\n",
        "- List of feature transformations for the raw feature"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the featurization summary JSON file locally\n",
        "best_run.download_file(\n",
        "    \"outputs/featurization_summary.json\", \"featurization_summary.json\"\n",
        ")\n",
        "\n",
        "# Render the JSON as a pandas DataFrame\n",
        "with open(\"featurization_summary.json\", \"r\") as f:\n",
        "    records = json.load(f)\n",
        "fs = pd.DataFrame.from_records(records)\n",
        "\n",
        "# View a summary of the featurization\n",
        "fs[\n",
        "    [\n",
        "        \"RawFeatureName\",\n",
        "        \"TypeDetected\",\n",
        "        \"Dropped\",\n",
        "        \"EngineeredFeatureCount\",\n",
        "        \"Transformations\",\n",
        "    ]\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272284093
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use the best fitted model from the AutoML Run to make forecasts for the test set. We will do batch scoring on the test dataset which should have the same schema as training dataset.\n",
        "\n",
        "The scoring will run on a remote compute. In this example, it will reuse the training compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_experiment = Experiment(ws, experiment_name + \"_test\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272284475
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieving forecasts from the model\n",
        "To run the forecast on the remote compute we will use a helper script: forecasting_script. This script contains the utility methods which will be used by the remote estimator. We copy the script to the project folder to upload it to remote compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "script_folder = os.path.join(os.getcwd(), \"forecast\")\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "shutil.copy(\"forecasting_script.py\", script_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272284854
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For brevity, we have created a function called run_forecast that submits the test data to the best model determined during the training run and retrieves forecasts. The test set is longer than the forecast horizon specified at train time, so the forecasting script uses a so-called rolling evaluation to generate predictions over the whole test set. A rolling evaluation iterates the forecaster over the test set, using the actuals in the test set to make lag features as needed. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from run_forecast import run_rolling_forecast\n",
        "\n",
        "remote_run = run_rolling_forecast(\n",
        "    test_experiment, compute_target, best_run, test, target_column_name\n",
        ")\n",
        "remote_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272285393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run.wait_for_completion(show_output=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272543838
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the prediction result for metrics calculation\n",
        "The test data with predictions are saved in artifact outputs/predictions.csv. You can download it and calculate some error metrics for the forecasts and vizualize the predictions vs. the actuals."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "remote_run.download_file(\"outputs/predictions.csv\", \"solar-panel-demand-predictions.csv\")\n",
        "fcst_df = pd.read_csv(\"solar-panel-demand-predictions.csv\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272544217
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the rolling forecast can contain multiple predictions for each date, each from a different forecast origin. For example, consider 2012-09-05:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fcst_df[fcst_df.date == \"2012-09-05\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272544506
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the forecast origin refers to the latest date of actuals available for a given forecast. The earliest origin in the rolling forecast, 2012-08-31, is the last day in the training data. For origin date 2012-09-01, the forecasts use actual recorded counts from the training data *and* the actual count recorded on 2012-09-01. Note that the model is not retrained for origin dates later than 2012-08-31, but the values for model features, such as lagged values of daily count are updated.\n",
        "\n",
        "Let's calculate the metrics over all rolling forecasts:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.automl.core.shared import constants\n",
        "from azureml.automl.runtime.shared.score import scoring\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# use automl metrics module\n",
        "scores = scoring.score_regression(\n",
        "    y_test=fcst_df[target_column_name],\n",
        "    y_pred=fcst_df[\"predicted\"],\n",
        "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET),\n",
        ")\n",
        "\n",
        "print(\"[Test data scores]\\n\")\n",
        "for key, value in scores.items():\n",
        "    print(\"{}:   {:.3f}\".format(key, value))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272545619
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details on what metrics are included and how they are calculated, please refer to [supported metrics](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#regressionforecasting-metrics). You could also calculate residuals, like described [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#residuals).\n",
        "\n",
        "The rolling forecast metric values are very high in comparison to the validation metrics reported by the AutoML job. What's going on here? We will investigate in the following cells!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecast versus actuals plot\n",
        "We will plot predictions and actuals on a time series plot. Since there are many forecasts for each date, we select the 14-day-ahead forecast from each forecast origin for our comparison."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fcst_df_h14 = (\n",
        "    fcst_df.groupby(\"forecast_origin\", as_index=False)\n",
        "    .last()\n",
        "    .drop(columns=[\"forecast_origin\"])\n",
        ")\n",
        "fcst_df_h14.set_index(time_column_name, inplace=True)\n",
        "plt.plot(fcst_df_h14[[target_column_name, \"predicted\"]])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(f\"Predicted vs. Actuals\")\n",
        "plt.legend([\"actual\", \"14-day-ahead forecast\"])\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272548006
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the plot, there are two clear issues:\n",
        "\n",
        "1. An anomalously low count value on October 29th, 2012.\n",
        "2. End-of-year holidays (Thanksgiving and Christmas) in late November and late December.\n",
        "\n",
        "What happened on Oct. 29th, 2012? That day, Hurricane Sandy brought severe storm surge flooding to the east coast of the United States, particularly around New York City. This is certainly an anomalous event that the model did not account for!\n",
        "\n",
        "As for the late year holidays, the model apparently did not learn to account for the full reduction of solar panels sold on these major holidays. The training data covers 2011 and early 2012, so the model fit only had access to a single occurrence of these holidays. This makes it challenging to resolve holiday effects; however, a larger AutoML model search may result in a better model that is more holiday-aware.\n",
        "\n",
        "If we filter the predictions prior to the Thanksgiving holiday and remove the anomalous day of 2012-10-29, the metrics are closer to validation levels:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "date_filter = (fcst_df.date != \"2012-10-29\") & (fcst_df.date < \"2012-11-22\")\n",
        "scores = scoring.score_regression(\n",
        "    y_test=fcst_df[date_filter][target_column_name],\n",
        "    y_pred=fcst_df[date_filter][\"predicted\"],\n",
        "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET),\n",
        ")\n",
        "\n",
        "print(\"[Test data scores (filtered)]\\n\")\n",
        "for key, value in scores.items():\n",
        "    print(\"{}:   {:.3f}\".format(key, value))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675272548242
        }
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "pygments_lexer": "ipython3",
    "task": "Forecasting",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "jialiu"
      }
    ],
    "vscode": {
      "interpreter": {
        "hash": "6bd77c88278e012ef31757c15997a7bea8c943977c43d6909403c00ae11d43ca"
      }
    },
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "python38-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "Remote"
    ],
    "version": 3,
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "tags": [
      "Forecasting"
    ],
    "datasets": [
      "BikeShare"
    ],
    "file_extension": ".py",
    "category": "tutorial",
    "framework": [
      "Azure ML AutoML"
    ],
    "friendly_name": "Forecasting BikeShare Demand",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}