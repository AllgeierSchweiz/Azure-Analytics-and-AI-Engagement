{"cells":[{"cell_type":"code","source":["%pip install mlflow\n%pip install dlt\n%pip install databricks-automl-runtime\n%pip install holidays\n%pip install xgboost==1.5.0\n%pip install sklearn\n%pip install numpy\n%pip install cloudpickle\n%pip install autocorrect\n%pip install better_profanity\n%pip install geopy"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"032d0aa9-47c4-480c-86a3-f7c34ea705af","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Sentiment Analytics On Delta Live Tables using ML\n1. **Application                :** Social Media Analytics <br/>\n2. **Usecase               :** Performing SQL analytics and Machine Learning algorithms on Twitter data coming incrementally from data lake.<br/>\n3. **Notebook Summary      :** This notebook is a part of social media analytics application which creates `Delta Live Tables`.<br/>\n4. **Notebook Description  :** Creates `Bronze, Silver and Gold` Delta Live tables to manage raw data, filter raw data and curated data respectively. This notebook also performs `ML operation` on Silver data to analyse sentiment analytics on Twitter Messages.\n\n\n[00_Initial_Setup](https://adb-1026867335382690.10.azuredatabricks.net/?o=1026867335382690#notebook/2296268478777632/command/916539995350847)<br/>\n[03_Sentiment_Analytics_On_Delta_Live_Tables](https://adb-1026867335382690.10.azuredatabricks.net/?o=1026867335382690#notebook/2296268478777530/command/916539995350918)<br/>\n[04_SQL_Analytics_On_Delta_Live_Tables](https://adb-1026867335382690.10.azuredatabricks.net/?o=1026867335382690#notebook/2296268478777426/command/916539995350920)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e843dd66-ae18-4902-bc44-4905e697d621","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import dlt\nimport mlflow\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import *\nimport re\nimport itertools\nfrom autocorrect import Speller\nspell = Speller(lang='en')\nfrom better_profanity import profanity\nimport numpy as np\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType, DecimalType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a62b6690-15ee-43bd-9ba5-51d4985fdb5a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Data Clean Up\n\nThe most common problem data engineers face is having to address dirty data. Tweets are notoriously hard to parse, but we've done our best and developed a user-defined function (UDF) that takes in a Tweet and performs some cleanup to ensure our Tweets conform to a common format for analysis. By making this part of the Delta Live Tables pipeline, we ensure this UDF can be run in a distributed manner on all of our data, accelerating our ETL process at scale considerably."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0cb9618-3782-4d17-8362-ce19f539649c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# defining regex\nconst_regex_hyperlink = r'https?:\\/\\/.\\S+'\nconst_regex_retweet = r'^RT[\\s]+'\nconst_regex_twitter_handle = r'@[\\w]*'\nconst_regex_word_sperator = \"([A-Z][a-z]+[^A-Z]*)\"\nconst_hashtag = r'#'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2c2f0f9-2104-4718-a7ee-a401cbd50d05","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#dictionary consisting of the contraction and the actual value\nApos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n\n\ndef clean_twitter_text(x):\n    # ignore non ascii\n    tweet = x.encode('ascii', 'ignore').decode('ascii')\n    # remove hyperlinks\n    tweet = re.sub(const_regex_hyperlink, \"\", tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(const_hashtag, '', tweet) \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(const_regex_retweet, '', tweet)    \n    # remove twitter handles (@user)\n    tweet = re.sub(const_regex_twitter_handle, '', tweet)\n\n    tweet = re.sub(\":\", '', tweet)\n    tweet = tweet.strip()\n    \n    #separate the words\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #One letter in a word should not be present more than twice in continuation\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #replace the contractions\n    for key,value in Apos_dict.items():\n        if key in tweet:\n            tweet=tweet.replace(key,value)\n    \n    return tweet\n\n#creating UDF \nclean_twitter_text_udf = udf(clean_twitter_text)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d10286f-552d-49c0-bb3d-8db8c91873fd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Incorporating our Sentiment Analysis ML Model\n\nOne goal we want to achieve from our pipeline is applying our newly created sentiment analysis ML model to the Tweets we're processing. With DLT this is as simple as declaring a UDF pointing to our model stored in our MLFlow registry. We can then leverage that UDF in our pipeline, passing in each of our Tweets as input and getting a sentiment score as an output. And extending this to streaming scenarios for real-time inference can be done just a couple of lines of code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74b6ae0c-cfe6-42e0-ae83-dd7a942651ed","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["const_model_run_id = spark.sql(\"select * from mlview\").collect()[0].value\nmodel_name = \"model\"\nmodel_uri = \"runs:/{run_id}/{model_name}\".format(run_id=const_model_run_id, model_name=model_name)\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='string')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"600d7584-cd61-451a-b982-8dba5497af10","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Twitter input data schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15f41db7-0632-44f0-9fe0-56012ffab2f7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Delta Live Table Setup\n\nAs a data engineer, we can use DLT pipelines to curate our raw Twitter data into useful data assets through filtering, augmentation, and other data processing techniques.\n\nYou can define a DLT pipeline in either Python or SQL. A DLT pipeline is declarative - you name and describe the tables you want to create, and then define them using familiar DataFrame syntax or SQL object DML.\n\nIn this pipeline, we'll move our Twitter data from our bronze to silver layer, filtering for specific hashtags we're interested in analyzing and applying our \"clean up\" function; and then enhancing it further by applying our sentiment ML model and creating some aggregated tables of hashtag counts by location for convenient consumption in tools like Power BI.\n\nOnce we've defined our pipeline in a notebook, we can configure it to run on a scheduled basis, continuously, or when triggered by the arrival of new data in our lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"102a9acb-834c-45b5-b1de-8209dfdc0c71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Twitter Schema\n\ntwitterSchema = StructType([    \n    StructField(\"time\",TimestampType(),True),\n    StructField(\"hashtag\",StringType(),True),\n    StructField(\"tweet\",StringType(),True),\n    StructField(\"city\",StringType(),True),    \n    StructField(\"username\",StringType(),True),\n    StructField(\"retweetcount\",IntegerType(),True),\n    StructField(\"favouritecount\",IntegerType(),True),\n    StructField(\"sentiment\",StringType(),True),\n    StructField(\"sentimentscore\",DecimalType(),True),\n    StructField(\"isretweet\",IntegerType(),True),\n    StructField(\"hourofday\",StringType(),True),\n    StructField(\"language\",StringType(),True) \n    ])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b77eb75-9b67-45f7-af4b-1ebd295a3c91","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["const_staging_path = \"/mnt/data-source/TwitterDataJsonSource/\"\n# Bronze Table Setup\n@dlt.table(\n    comment=\"Raw data\",\n    table_properties={\n    \"quality\": \"bronze\"\n    }    \n)\n@dlt.expect_or_drop(\"valid_city\", \"City IS NOT NULL\")\ndef bronze_twitter_historical_data():\n        return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .schema(twitterSchema)\n      .load(const_staging_path)\n  )\n\n\n# Silver Table Setup\n@dlt.create_table(\n  comment=\"Preparing\",  \n  table_properties={\n    \"quality\": \"silver\"\n  }    \n)\ndef silver_twitter_historical_data():\n  return dlt.readStream(\"bronze_twitter_historical_data\").filter(col(\"tweet\").contains(\"#fashion\") | col(\"tweet\").contains(\"#beach\")| col(\"tweet\").contains(\"#entertainment\")| col(\"tweet\").contains(\"#gogreen\")| col(\"tweet\").contains(\"#sustainablefashion\")| col(\"tweet\").contains(\"#futuretech\")).withColumn(\"tweet\", clean_twitter_text_udf('tweet')).withColumn(\"Date\",to_date(\"time\"))\n\n\n\n# Gold Table Setup\n@dlt.create_table(\n  comment=\"Predicting\", \n  partition_cols = [\"Date\"],\n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_data():\n  return dlt.read(\"silver_twitter_historical_data\").withColumn('MLSentiment', loaded_model('tweet'))\n\n\n\n# Gold (Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_hashtag_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"hashtag\").count()\n\n\n\n# Gold (Retweet) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_retweetcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").selectExpr(\"sum(cast(retweetcount as int)) RetweetCount\")\n\n\n\n# Gold (City vs Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_city_hashtagcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"city\", \"hashtag\").count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49aca32e-ac0c-44d1-9172-52a8bec7b8e7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03_Sentiment_Analytics_On_Delta_Live_Tables.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2734317015725920}},"nbformat":4,"nbformat_minor":0}
