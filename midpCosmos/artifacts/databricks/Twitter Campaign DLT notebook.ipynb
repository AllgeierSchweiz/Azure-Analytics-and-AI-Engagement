{"cells":[{"cell_type":"code","source":["%pip install mlflow\n%pip install dlt\n%pip install databricks-automl-runtime\n%pip install holidays\n%pip install xgboost==1.5.0\n%pip install sklearn\n%pip install numpy\n%pip install cloudpickle\n%pip install autocorrect\n%pip install better_profanity\n%pip install geopy\n%pip install category-encoders"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"881ecbeb-9ab7-4363-9bc3-c3e1d0edaa9a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Sentiment Analytics On Delta Live Tables using ML\n1. **Application                :** Social Media Analytics <br/>\n2. **Usecase               :** Performing SQL analytics and Machine Learning algorithms on Twitter data coming incrementally from data lake.<br/>\n3. **Notebook Summary      :** This notebook is a part of social media analytics application which creates `Delta Live Tables`.<br/>\n4. **Notebook Description  :** Creates `Bronze, Silver and Gold` Delta Live tables to manage raw data, filter raw data and curated data respectively. This notebook also performs `ML operation` on Silver data to analyze sentiment analytics on Twitter Messages."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32025f9c-17cb-45bc-81e6-af810e57d777","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import dlt\nimport mlflow\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import *\nimport re\nimport itertools\nfrom autocorrect import Speller\nspell = Speller(lang='en')\nfrom better_profanity import profanity\nimport numpy as np\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType, DecimalType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8113f3cb-8339-4a6c-a6a9-0a61b6036ed9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Data Clean Up\n\nThe most common problem data engineers face is having to address unclean data. Tweets are notoriously hard to parse, but we've done our best and developed a user-defined function (UDF) that takes in a Tweet and performs some cleanup to ensure our Tweets conform to a common format for analysis. By making this part of the Delta Live Tables pipeline, we ensure this UDF can be run in a distributed manner on all of our data, accelerating our ETL process at scale considerably."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab2ee14c-c40d-4702-84f0-5a23d791ea9d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# defining regex\nconst_regex_hyperlink = r'https?:\\/\\/.\\S+'\nconst_regex_retweet = r'^RT[\\s]+'\nconst_regex_twitter_handle = r'@[\\w]*'\nconst_regex_word_sperator = \"([A-Z][a-z]+[^A-Z]*)\"\nconst_hashtag = r'#'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbff1e5b-2467-4ff1-a53e-84cce5da049b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#dictionary consisting of the contraction and the actual value\nApos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n\n\ndef clean_twitter_text(x):\n    # ignore non ascii\n    tweet = x.encode('ascii', 'ignore').decode('ascii')\n    # remove hyperlinks\n    tweet = re.sub(const_regex_hyperlink, \"\", tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(const_hashtag, '', tweet) \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(const_regex_retweet, '', tweet)    \n    # remove twitter handles (@user)\n    tweet = re.sub(const_regex_twitter_handle, '', tweet)\n\n    tweet = re.sub(\":\", '', tweet)\n    tweet = tweet.strip()\n    \n    #separate the words\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #One letter in a word should not be present more than twice in continuation\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #replace the contractions\n    for key,value in Apos_dict.items():\n        if key in tweet:\n            tweet=tweet.replace(key,value)\n    \n    return tweet\n\n#creating UDF \nclean_twitter_text_udf = udf(clean_twitter_text)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db3678be-9d25-4d6b-963a-7e5e05c1845a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Incorporating our Sentiment Analysis ML Model\n\nOne goal we want to achieve from our pipeline is applying our newly created sentiment analysis ML model to the Tweets we're processing. With DLT this is as simple as declaring a UDF pointing to our model stored in our MLFlow registry. We can then leverage that UDF in our pipeline, passing in each of our Tweets as input and getting a sentiment score as an output. And extending this to streaming scenarios for real-time inference can be done just a couple of lines of code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a4e87f0-77dd-4073-9d7c-a9e4d5e6102f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["const_model_run_id = spark.sql(\"select * from mlview\").collect()[0].value\nmodel_name = \"model\"\nmodel_uri = \"runs:/{run_id}/{model_name}\".format(run_id=const_model_run_id, model_name=model_name)\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='string')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"947cee22-8ef2-4bf3-b2f4-4843ced42568","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Twitter input data schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3a749f8-e79b-46e9-9912-3c66ce9c4d19","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Delta Live Table Setup\n\nAs a data engineer, we can use DLT pipelines to curate our raw Twitter data into useful data assets through filtering, augmentation, and other data processing techniques.\n\nYou can define a DLT pipeline in either Python or SQL. A DLT pipeline is declarative - you name and describe the tables you want to create, and then define them using familiar DataFrame syntax or SQL object DML.\n\nIn this pipeline, we'll move our Twitter data from our bronze to silver layer, filtering for specific hashtags we're interested in analyzing and applying our \"clean up\" function; and then enhancing it further by applying our sentiment ML model and creating some aggregated tables of hashtag counts by location for convenient consumption in tools like Power BI.\n\nOnce we've defined our pipeline in a notebook, we can configure it to run on a scheduled basis, continuously, or when triggered by the arrival of new data in our lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3783a06-f5bd-4a4d-81d5-031590ea3344","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Twitter Schema\n\ntwitterSchema = StructType([    \n    StructField(\"time\",TimestampType(),True),\n    StructField(\"hashtag\",StringType(),True),\n    StructField(\"tweet\",StringType(),True),\n    StructField(\"city\",StringType(),True),    \n    StructField(\"username\",StringType(),True),\n    StructField(\"retweetcount\",IntegerType(),True),\n    StructField(\"favouritecount\",IntegerType(),True),\n    StructField(\"sentiment\",StringType(),True),\n    StructField(\"sentimentscore\",DecimalType(),True),\n    StructField(\"isretweet\",IntegerType(),True),\n    StructField(\"hourofday\",StringType(),True),\n    StructField(\"language\",StringType(),True) \n    ])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0460b9cb-7c12-4495-9d17-d06b14806fe1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["const_staging_path = \"/mnt/data-source/TwitterDataJsonSource/\"\n# Bronze Table Setup\n@dlt.table(\n    comment=\"Raw data\",\n    table_properties={\n    \"quality\": \"bronze\"\n    }    \n)\n@dlt.expect_or_drop(\"valid_city\", \"City IS NOT NULL\")\ndef bronze_twitter_historical_data():\n        return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .schema(twitterSchema)\n      .load(const_staging_path)\n  )\n\n\n# Silver Table Setup\n@dlt.create_table(\n  comment=\"Preparing\",  \n  table_properties={\n    \"quality\": \"silver\"\n  }    \n)\ndef silver_twitter_historical_data():\n  return dlt.readStream(\"bronze_twitter_historical_data\").filter(col(\"tweet\").contains(\"#fashion\") | col(\"tweet\").contains(\"#beach\")| col(\"tweet\").contains(\"#entertainment\")| col(\"tweet\").contains(\"#gogreen\")| col(\"tweet\").contains(\"#sustainablefashion\")| col(\"tweet\").contains(\"#futuretech\")).withColumn(\"tweet\", clean_twitter_text_udf('tweet')).withColumn(\"Date\",to_date(\"time\"))\n\n\n\n# Gold Table Setup\n@dlt.create_table(\n  comment=\"Predicting\", \n  partition_cols = [\"Date\"],\n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_data():\n  return dlt.read(\"silver_twitter_historical_data\").withColumn('MLSentiment', loaded_model('tweet'))\n\n\n\n# Gold (Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_hashtag_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"hashtag\").count()\n\n\n\n# Gold (Retweet) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_retweetcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").selectExpr(\"sum(cast(retweetcount as int)) RetweetCount\")\n\n\n\n# Gold (City vs Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_city_hashtagcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"city\", \"hashtag\").count()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfe1caa8-0807-4c4c-a349-0e65c803c950","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Campaign Analytics\n\n1. **Usecase               :** Performing Campaign analytics on static campaign data coming from snowflake in azure container.<br/>\n2. **Notebook Summary      :** This notebook is a part of campaign analytics application which perform `campaign analytics using various pyspark capability`.\n3. **Notebook Description  :** Performing Campaign Analytics on Azure Container Files.\n\n\n###Feature List\n1. Data Profiling\n2. Total Revenue, Total Revenue Target & Profit \n3. Campaign Run by Per Week \n4. Total Profit by Country Per Week\n5. Top Loss-Making Campaign \n\nThe bronze data received for processing is already curated. So, we will derive gold tables from bronze tables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f945b539-286c-4596-a3c0-2e5ed2154081","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98d7cc1f-b1ba-4e55-a755-00eed993e6d2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum as _sum\nfrom pyspark.sql.functions import mean as _mean\nfrom pyspark.sql.functions import max as _max\nfrom pyspark.sql.functions import min as _min\nimport pyspark.sql.functions as func\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdf8cd2d-e546-45a2-aa6a-400d69191291","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Define the Schema for the input file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29f80f2e-b241-47c0-a25c-943ac4e81d55","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["campaignSchema = StructType([    \n    StructField(\"Region\",StringType(),True),\n    StructField(\"Country\",StringType(),True),\n    StructField(\"ProductCategory\",StringType(),True),\n    StructField(\"Campaign_ID\",IntegerType(),True),    \n    StructField(\"Campaign_Name\",StringType(),True),\n    StructField(\"Qualification\",StringType(),True),\n    StructField(\"Qualification_Number\",StringType(),True),\n    StructField(\"Response_Status\",StringType(),True),\n    StructField(\"Responses\",FloatType(),True),\n    StructField(\"Cost\",FloatType(),True),\n    StructField(\"Revenue\",FloatType(),True),\n    StructField(\"ROI\",FloatType(),True),\n    StructField(\"Lead_Generation\",StringType(),True),\n    StructField(\"Revenue_Target\",FloatType(),True),\n    StructField(\"Campaign_Tactic\",StringType(),True),\n    StructField(\"Customer_Segment\",StringType(),True),\n    StructField(\"Status\",StringType(),True),\n    StructField(\"Profit\",FloatType(),True),\n    StructField(\"Marketing_Cost\",FloatType(),True),\n    StructField(\"CampaignID\",IntegerType(),True),\n    StructField(\"CampDate\",DateType(),True), \n    StructField(\"SORTED_ID\",IntegerType(),True)])\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8919ad73-af69-4357-b6ee-037ae9fc0146","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Load the Campaign Dataset from Data Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9954ccc-8870-41a7-954a-7a7d12b86ba9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Bronze Table Setup\n@dlt.table(comment=\"Raw data\")\ndef bronze_campaign_data():\n#   return (spark.table(\"campaign.campaign_source\"))\n  return (spark.read.format(\"csv\").option(\"header\",True).schema(campaignSchema).load(\"/mnt/data-source/Campaign Data/campaign-data2.csv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e3e9b62-f05c-4e8e-a546-7a727c4fbfa9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Total Revenue, Total Revenue Target & Profit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"234b6725-ad44-4001-9163-21075e934ed2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_country_wise_revenue():\n    \n    df = dlt.read(\"bronze_campaign_data\").groupBy(\"Country\",\"Region\").agg(_sum(\"Revenue\").alias(\"Total_Revenue\"), _sum(\"Revenue_Target\").alias(\"Total_Revenue_Target\"),_sum(\"Profit\").alias(\"Total_Profit\"),_max(\"Cost\").alias(\"Max_Cost\"),_min(\"Cost\").alias(\"Min_Cost\"))\n    df = df.withColumn(\"Total_Revenue\", func.round(df[\"Total_Revenue\"],2)).withColumn(\"Total_Revenue_Target\", func.round(df[\"Total_Revenue_Target\"], 2)).withColumn(\"Total_Profit\", func.round(df[\"Total_Profit\"], 2))\n    return df\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc1e242b-ab17-4b78-85d3-4b59f57cd435","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Campaign Run by Per Week"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb05fdf0-0553-4514-8ef7-00cd297ad39b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_campaign_per_week():\n    return dlt.read(\"bronze_campaign_data\") \\\n    .groupBy(\n      \"Campaign_Name\",\n      window(\"CampDate\", \"1 week\")) \\\n    .count().orderBy(col(\"count\").desc())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1327a9e-0d26-40a0-b5c6-f898e05fd96e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Total Profit by Country Per Week"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80f2bf50-b629-44dc-8c98-6b94e85db9ab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_Total_Profit_by_Country_Per_Week():\n    return dlt.read(\"bronze_campaign_data\").select(\"Region\",\"Country\", \"Cost\", \"Profit\",\"CampDate\") \\\n                     .groupBy(window(col(\"CampDate\"), \"7 days\"), col(\"Country\")) \\\n                     .agg(sum(\"Profit\").alias('Total_Profit'),) \\\n                            .orderBy(col(\"window.start\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4e1991f-5c89-4208-8215-8660ffff279b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Top Loss-Making Campaign"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44e6c411-5ad9-413a-b237-07425db8bfea","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_Top_Loss_Making_Campaign():\n    loss = dlt.read(\"bronze_campaign_data\").select(\"Campaign_Name\",\"Profit\").filter(F.col(\"Profit\") < 0)\n    loss = loss.withColumn(\"Loss_Count\", F.when((F.col('Profit') < 0 ) , F.lit(1)).otherwise(F.lit(0)))\n#     loss = loss.groupBy('Campaign_Name').sum('Loss_Count')\n    return loss"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b71da431-79ca-4b77-b598-217a7cbde349","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Campaign Powered by Twitter"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4162250-ba41-4615-80ca-41083746eb67","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["   \n@dlt.table(\n    comment=\"Campaign powered by Twitter\",\n    table_properties={\n    \"quality\": \"gold\"\n    }    \n) \n\ndef Sentiment_Campaign_Analytics():\n    return (\n        spark.sql(\"\"\"\n         select a.* from live.gold_twitter_historical_city_hashtagcount_data a,\n              live.gold_campaign_per_week b\n    where replace(a.hashtag, '#', '') = b.campaign_name\n        \"\"\")\n    )\n    \n\n       "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d9327e8-7f14-4e88-8a73-4c83046e2bcb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Twitter Campaign DLT notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2734317015726155}},"nbformat":4,"nbformat_minor":0}
