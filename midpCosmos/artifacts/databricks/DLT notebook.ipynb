{"cells":[{"cell_type":"code","source":["%pip install mlflow\n%pip install dlt\n%pip install databricks-automl-runtime\n%pip install holidays\n%pip install xgboost==1.5.0\n%pip install sklearn\n%pip install numpy\n%pip install cloudpickle\n%pip install autocorrect\n%pip install better_profanity\n%pip install geopy\n%pip install category-encoders"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54e6715d-d347-4f3a-a914-0db056ac8405","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-2.1.1-py3-none-any.whl (16.7 MB)\nRequirement already satisfied: gunicorn<21 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (20.1.0)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (6.0)\nRequirement already satisfied: pandas<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nRequirement already satisfied: shap<1,>=0.40 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.41.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<6,>=3.7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (4.11.3)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: cloudpickle<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.0.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\nRequirement already satisfied: pytz<2023 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nCollecting sqlalchemy<2,>=1.4.0\n  Downloading SQLAlchemy-1.4.46-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting alembic<2\n  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nRequirement already satisfied: Flask<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.1.2)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.3.4)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pyarrow<11,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: packaging<23 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.17.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.9/site-packages (from alembic<2->mlflow) (1.2.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (1.26.9)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (0.58.0)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.55.1)\nRequirement already satisfied: tqdm>4.25.0 in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (4.64.0)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.0.7)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /databricks/python3/lib/python3.9/site-packages (from numba->shap<1,>=0.40->mlflow) (0.38.0)\nInstalling collected packages: greenlet, sqlalchemy, querystring-parser, docker, alembic, mlflow\nSuccessfully installed alembic-1.9.2 docker-6.0.1 greenlet-2.0.2 mlflow-2.1.1 querystring-parser-1.2.4 sqlalchemy-1.4.46\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting dlt\n  Downloading dlt-0.2.3-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: Numpy in /databricks/python3/lib/python3.9/site-packages (from dlt) (1.21.5)\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\nRequirement already satisfied: keras in /databricks/python3/lib/python3.9/site-packages (from dlt) (2.10.0)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.9/site-packages (from dlt) (3.5.1)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (from dlt) (1.0.2)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (21.3)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (9.0.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->dlt) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (2.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (1.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (22.10.26)\nCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (2.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (61.2.0)\nCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.6.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.42.0)\nCollecting keras\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\nRequirement already satisfied: google-pasta>=0.1.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.3.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.28.0)\nRequirement already satisfied: libclang>=13.0.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (14.0.6)\nRequirement already satisfied: absl-py>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.0.0)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.12.1)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.19.4)\nRequirement already satisfied: typing-extensions>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (4.1.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.4.0)\nRequirement already satisfied: h5py>=2.9.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.6.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->dlt) (0.37.1)\nRequirement already satisfied: markdown>=2.6.8 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (3.3.4)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (1.8.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (2.0.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (2.27.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (1.33.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (0.2.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->dlt) (1.3.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (0.4.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (2021.10.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->dlt) (3.2.0)\nInstalling collected packages: tensorflow-estimator, tensorboard, keras, tensorflow, dlt\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.10.0\n    Not uninstalling tensorflow-estimator at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'tensorflow-estimator'. No files were found to uninstall.\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.0\n    Not uninstalling tensorboard at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'tensorboard'. No files were found to uninstall.\n  Attempting uninstall: keras\n    Found existing installation: keras 2.10.0\n    Not uninstalling keras at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'keras'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cpu 2.10.0 requires keras<2.11,>=2.10.0, but you have keras 2.11.0 which is incompatible.\ntensorflow-cpu 2.10.0 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.11.2 which is incompatible.\ntensorflow-cpu 2.10.0 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.11.0 which is incompatible.\nSuccessfully installed dlt-0.2.3 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: databricks-automl-runtime in /databricks/python3/lib/python3.9/site-packages (0.2.13)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: holidays in /databricks/python3/lib/python3.9/site-packages (0.16)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.8.2)\nRequirement already satisfied: convertdate>=2.3.0 in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.4.0)\nRequirement already satisfied: hijri-converter in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.2.4)\nRequirement already satisfied: korean-lunar-calendar in /databricks/python3/lib/python3.9/site-packages (from holidays) (0.3.1)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /databricks/python3/lib/python3.9/site-packages (from convertdate>=2.3.0->holidays) (0.5.11)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil->holidays) (1.16.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting xgboost==1.5.0\n  Downloading xgboost-1.5.0-py3-none-manylinux2014_x86_64.whl (173.5 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost==1.5.0) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost==1.5.0) (1.7.3)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-1.5.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting sklearn\n  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n    ERROR: Command errored out with exit status 1:\n     command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-2blrrqr8\n         cwd: /tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/\n    Complete output (18 lines):\n    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n    rather than 'sklearn' for pip commands.\n    \n    Here is how to fix this error in the main use cases:\n    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n    - if the 'sklearn' package is used by one of your dependencies,\n      it would be great if you take some time to track which package uses\n      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n    - as a last resort, set the environment variable\n      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n    \n    More information is available at\n    https://github.com/scikit-learn/sklearn-pypi-package\n    \n    If the previous advice does not cover your use case, feel free to report it at\n    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n    ----------------------------------------\nWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (from sklearn) (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.5)\nBuilding wheels for collected packages: sklearn\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=b5fe28490b8472cc0e635cb4c878b1dd0e37264b11bf90d95a9a934924a1d242\n  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\nSuccessfully built sklearn\nInstalling collected packages: sklearn\nSuccessfully installed sklearn-0.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (1.21.5)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.9/site-packages (2.0.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting autocorrect\n  Downloading autocorrect-2.6.1.tar.gz (622 kB)\nBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py): started\n  Building wheel for autocorrect (setup.py): finished with status 'done'\n  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=b71e46458174c26b3fab93c21f14529f5295131b886acef6c0e638eeba8572ce\n  Stored in directory: /root/.cache/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting better_profanity\n  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\nInstalling collected packages: better-profanity\nSuccessfully installed better-profanity-0.7.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting geopy\n  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\nCollecting geographiclib<3,>=1.52\n  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\nInstalling collected packages: geographiclib, geopy\nSuccessfully installed geographiclib-2.0 geopy-2.3.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: category-encoders in /databricks/python3/lib/python3.9/site-packages (2.5.1.post0)\nRequirement already satisfied: scikit-learn>=0.20.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.0.2)\nRequirement already satisfied: scipy>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.7.3)\nRequirement already satisfied: patsy>=0.5.1 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (0.5.2)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.4.2)\nRequirement already satisfied: numpy>=1.14.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.21.5)\nRequirement already satisfied: statsmodels>=0.9.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (0.13.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0.5->category-encoders) (2021.3)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (1.1.0)\nRequirement already satisfied: packaging>=21.3 in /databricks/python3/lib/python3.9/site-packages (from statsmodels>=0.9.0->category-encoders) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category-encoders) (3.0.4)\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-2.1.1-py3-none-any.whl (16.7 MB)\nRequirement already satisfied: gunicorn<21 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (20.1.0)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (6.0)\nRequirement already satisfied: pandas<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nRequirement already satisfied: shap<1,>=0.40 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.41.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<6,>=3.7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (4.11.3)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: cloudpickle<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.0.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\nRequirement already satisfied: pytz<2023 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nCollecting sqlalchemy<2,>=1.4.0\n  Downloading SQLAlchemy-1.4.46-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting alembic<2\n  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nRequirement already satisfied: Flask<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.1.2)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.3.4)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pyarrow<11,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: packaging<23 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.17.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.9/site-packages (from alembic<2->mlflow) (1.2.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (1.26.9)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.9/site-packages (from docker<7,>=4.0.0->mlflow) (0.58.0)\nRequirement already satisfied: itsdangerous>=0.24 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow) (2.0.1)\nRequirement already satisfied: Werkzeug>=0.15 in /databricks/python3/lib/python3.9/site-packages (from Flask<3->mlflow) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.55.1)\nRequirement already satisfied: tqdm>4.25.0 in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (4.64.0)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.9/site-packages (from shap<1,>=0.40->mlflow) (0.0.7)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /databricks/python3/lib/python3.9/site-packages (from numba->shap<1,>=0.40->mlflow) (0.38.0)\nInstalling collected packages: greenlet, sqlalchemy, querystring-parser, docker, alembic, mlflow\nSuccessfully installed alembic-1.9.2 docker-6.0.1 greenlet-2.0.2 mlflow-2.1.1 querystring-parser-1.2.4 sqlalchemy-1.4.46\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting dlt\n  Downloading dlt-0.2.3-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: Numpy in /databricks/python3/lib/python3.9/site-packages (from dlt) (1.21.5)\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\nRequirement already satisfied: keras in /databricks/python3/lib/python3.9/site-packages (from dlt) (2.10.0)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.9/site-packages (from dlt) (3.5.1)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (from dlt) (1.0.2)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (21.3)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->dlt) (9.0.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->dlt) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (2.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->dlt) (1.1.0)\nRequirement already satisfied: flatbuffers>=2.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (22.10.26)\nCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (2.1.1)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (61.2.0)\nCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.6.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.42.0)\nCollecting keras\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\nRequirement already satisfied: google-pasta>=0.1.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.3.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.28.0)\nRequirement already satisfied: libclang>=13.0.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (14.0.6)\nRequirement already satisfied: absl-py>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.0.0)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (1.12.1)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.19.4)\nRequirement already satisfied: typing-extensions>=3.6.6 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (4.1.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (0.4.0)\nRequirement already satisfied: h5py>=2.9.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow->dlt) (3.6.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow->dlt) (0.37.1)\nRequirement already satisfied: markdown>=2.6.8 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (3.3.4)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (1.8.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (2.0.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (2.27.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /databricks/python3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow->dlt) (1.33.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (0.2.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /databricks/python3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->dlt) (1.3.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->dlt) (0.4.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->dlt) (2021.10.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->dlt) (3.2.0)\nInstalling collected packages: tensorflow-estimator, tensorboard, keras, tensorflow, dlt\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.10.0\n    Not uninstalling tensorflow-estimator at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'tensorflow-estimator'. No files were found to uninstall.\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.0\n    Not uninstalling tensorboard at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'tensorboard'. No files were found to uninstall.\n  Attempting uninstall: keras\n    Found existing installation: keras 2.10.0\n    Not uninstalling keras at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d\n    Can't uninstall 'keras'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cpu 2.10.0 requires keras<2.11,>=2.10.0, but you have keras 2.11.0 which is incompatible.\ntensorflow-cpu 2.10.0 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.11.2 which is incompatible.\ntensorflow-cpu 2.10.0 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.11.0 which is incompatible.\nSuccessfully installed dlt-0.2.3 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: databricks-automl-runtime in /databricks/python3/lib/python3.9/site-packages (0.2.13)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: holidays in /databricks/python3/lib/python3.9/site-packages (0.16)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.8.2)\nRequirement already satisfied: convertdate>=2.3.0 in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.4.0)\nRequirement already satisfied: hijri-converter in /databricks/python3/lib/python3.9/site-packages (from holidays) (2.2.4)\nRequirement already satisfied: korean-lunar-calendar in /databricks/python3/lib/python3.9/site-packages (from holidays) (0.3.1)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /databricks/python3/lib/python3.9/site-packages (from convertdate>=2.3.0->holidays) (0.5.11)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil->holidays) (1.16.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting xgboost==1.5.0\n  Downloading xgboost-1.5.0-py3-none-manylinux2014_x86_64.whl (173.5 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost==1.5.0) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost==1.5.0) (1.7.3)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-1.5.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting sklearn\n  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n    ERROR: Command errored out with exit status 1:\n     command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-99cda23c-ad5e-457b-84da-a5d651974b5d/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-2blrrqr8\n         cwd: /tmp/pip-install-1hfzjv76/sklearn_c09a54aa81334bd695b7cda6af55842a/\n    Complete output (18 lines):\n    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n    rather than 'sklearn' for pip commands.\n    \n    Here is how to fix this error in the main use cases:\n    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n    - if the 'sklearn' package is used by one of your dependencies,\n      it would be great if you take some time to track which package uses\n      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n    - as a last resort, set the environment variable\n      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n    \n    More information is available at\n    https://github.com/scikit-learn/sklearn-pypi-package\n    \n    If the previous advice does not cover your use case, feel free to report it at\n    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n    ----------------------------------------\nWARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n  Downloading sklearn-0.0.tar.gz (1.1 kB)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (from sklearn) (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.5)\nBuilding wheels for collected packages: sklearn\n  Building wheel for sklearn (setup.py): started\n  Building wheel for sklearn (setup.py): finished with status 'done'\n  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=b5fe28490b8472cc0e635cb4c878b1dd0e37264b11bf90d95a9a934924a1d242\n  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\nSuccessfully built sklearn\nInstalling collected packages: sklearn\nSuccessfully installed sklearn-0.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (1.21.5)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.9/site-packages (2.0.0)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting autocorrect\n  Downloading autocorrect-2.6.1.tar.gz (622 kB)\nBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py): started\n  Building wheel for autocorrect (setup.py): finished with status 'done'\n  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=b71e46458174c26b3fab93c21f14529f5295131b886acef6c0e638eeba8572ce\n  Stored in directory: /root/.cache/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting better_profanity\n  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\nInstalling collected packages: better-profanity\nSuccessfully installed better-profanity-0.7.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting geopy\n  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\nCollecting geographiclib<3,>=1.52\n  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\nInstalling collected packages: geographiclib, geopy\nSuccessfully installed geographiclib-2.0 geopy-2.3.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: category-encoders in /databricks/python3/lib/python3.9/site-packages (2.5.1.post0)\nRequirement already satisfied: scikit-learn>=0.20.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.0.2)\nRequirement already satisfied: scipy>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.7.3)\nRequirement already satisfied: patsy>=0.5.1 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (0.5.2)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.4.2)\nRequirement already satisfied: numpy>=1.14.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (1.21.5)\nRequirement already satisfied: statsmodels>=0.9.0 in /databricks/python3/lib/python3.9/site-packages (from category-encoders) (0.13.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0.5->category-encoders) (2021.3)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (1.1.0)\nRequirement already satisfied: packaging>=21.3 in /databricks/python3/lib/python3.9/site-packages (from statsmodels>=0.9.0->category-encoders) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category-encoders) (3.0.4)\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Sentiment Analytics On Delta Live Tables using ML\n1. **Application                :** Social Media Analytics <br/>\n2. **Usecase               :** Performing SQL analytics and Machine Learning algorithms on Twitter data coming incrementally from data lake.<br/>\n3. **Notebook Summary      :** This notebook is a part of social media analytics application which creates `Delta Live Tables`.<br/>\n4. **Notebook Description  :** Creates `Bronze, Silver and Gold` Delta Live tables to manage raw data, filter raw data and curated data respectively. This notebook also performs `ML operation` on Silver data to analyze sentiment analytics on Twitter Messages."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b870fda4-24d3-4a3f-8e20-da74c45e244f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import dlt\nimport mlflow\nfrom pyspark.sql.functions import struct\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import *\nimport re\nimport itertools\nfrom autocorrect import Speller\nspell = Speller(lang='en')\nfrom better_profanity import profanity\nimport numpy as np\nfrom delta.tables import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType, DecimalType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"622d19de-d02c-4a12-9881-114200af16c7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Data Clean Up\n\nThe most common problem data engineers face is having to address dirty data. Tweets are notoriously hard to parse, but we've done our best and developed a user-defined function (UDF) that takes in a Tweet and performs some cleanup to ensure our Tweets conform to a common format for analysis. By making this part of the Delta Live Tables pipeline, we ensure this UDF can be run in a distributed manner on all of our data, accelerating our ETL process at scale considerably."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd03236c-257f-4534-8f19-5d644f8cde3e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# defining regex\nconst_regex_hyperlink = r'https?:\\/\\/.\\S+'\nconst_regex_retweet = r'^RT[\\s]+'\nconst_regex_twitter_handle = r'@[\\w]*'\nconst_regex_word_sperator = \"([A-Z][a-z]+[^A-Z]*)\"\nconst_hashtag = r'#'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36ecd29c-252d-4ca3-b3db-a041c2de613d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#dictionary consisting of the contraction and the actual value\nApos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n           \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n\n\ndef clean_twitter_text(x):\n    # ignore non ascii\n    tweet = x.encode('ascii', 'ignore').decode('ascii')\n    # remove hyperlinks\n    tweet = re.sub(const_regex_hyperlink, \"\", tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(const_hashtag, '', tweet) \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(const_regex_retweet, '', tweet)    \n    # remove twitter handles (@user)\n    tweet = re.sub(const_regex_twitter_handle, '', tweet)\n\n    tweet = re.sub(\":\", '', tweet)\n    tweet = tweet.strip()\n    \n    #separate the words\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #One letter in a word should not be present more than twice in continuation\n    tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n    \n    #replace the contractions\n    for key,value in Apos_dict.items():\n        if key in tweet:\n            tweet=tweet.replace(key,value)\n    \n    return tweet\n\n#creating UDF \nclean_twitter_text_udf = udf(clean_twitter_text)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09d0a6f9-5df8-4c1c-8218-8b4343b68082","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Incorporating our Sentiment Analysis ML Model\n\nOne goal we want to achieve from our pipeline is applying our newly created sentiment analysis ML model to the Tweets we're processing. With DLT this is as simple as declaring a UDF pointing to our model stored in our MLFlow registry. We can then leverage that UDF in our pipeline, passing in each of our Tweets as input and getting a sentiment score as an output. And extending this to streaming scenarios for real-time inference can be done just a couple of lines of code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09d84142-3b0c-454f-809f-ba23e07e3853","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["const_model_run_id = spark.sql(\"select * from mlview\").collect()[0].value\nmodel_name = \"model\"\nmodel_uri = \"runs:/{run_id}/{model_name}\".format(run_id=const_model_run_id, model_name=model_name)\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='string')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fda27d7-68c7-41dd-8e50-b28ebded1e07","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"2023/02/03 17:16:31 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - cloudpickle (current: 2.0.0, required: cloudpickle==2.2.1)\n - xgboost (current: 1.5.0, required: xgboost==0.90)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n2023/02/03 17:16:31 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n2023/02/03 17:16:31 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2023/02/03 17:16:31 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - cloudpickle (current: 2.0.0, required: cloudpickle==2.2.1)\n - xgboost (current: 1.5.0, required: xgboost==0.90)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n2023/02/03 17:16:31 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n2023/02/03 17:16:31 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Twitter input data schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b72b38b-f9e4-4f83-859d-8d28166171e7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Delta Live Table Setup\n\nAs a data engineer, we can use DLT pipelines to curate our raw Twitter data into useful data assets through filtering, augmentation, and other data processing techniques.\n\nYou can define a DLT pipeline in either Python or SQL. A DLT pipeline is declarative - you name and describe the tables you want to create, and then define them using familiar DataFrame syntax or SQL object DML.\n\nIn this pipeline, we'll move our Twitter data from our bronze to silver layer, filtering for specific hashtags we're interested in analyzing and applying our \"clean up\" function; and then enhancing it further by applying our sentiment ML model and creating some aggregated tables of hashtag counts by location for convenient consumption in tools like Power BI.\n\nOnce we've defined our pipeline in a notebook, we can configure it to run on a scheduled basis, continuously, or when triggered by the arrival of new data in our lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"394908a6-5844-46ca-89ae-30e86fc4ec8f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Twitter Schema\n\ntwitterSchema = StructType([    \n    StructField(\"time\",TimestampType(),True),\n    StructField(\"hashtag\",StringType(),True),\n    StructField(\"tweet\",StringType(),True),\n    StructField(\"city\",StringType(),True),    \n    StructField(\"username\",StringType(),True),\n    StructField(\"retweetcount\",IntegerType(),True),\n    StructField(\"favouritecount\",IntegerType(),True),\n    StructField(\"sentiment\",StringType(),True),\n    StructField(\"sentimentscore\",DecimalType(),True),\n    StructField(\"isretweet\",IntegerType(),True),\n    StructField(\"hourofday\",StringType(),True),\n    StructField(\"language\",StringType(),True) \n    ])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34619185-107b-4aa3-8b7f-a5726b724e57","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nls /mnt/data-source/TwitterDataJsonSource/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65c1c5d9-c0f7-4e59-9bc3-3a1a2090bc2f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\">\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:338)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:267)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:207)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:205)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:385)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1241)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1208)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1166)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:623)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:97)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$3(DBUtilsCore.scala:218)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$2(DBUtilsCore.scala:215)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:139)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:209)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:1)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:43)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:45)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:47)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw.&lt;init&gt;(command-3848542693110191:49)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw.&lt;init&gt;(command-3848542693110191:51)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read.&lt;init&gt;(command-3848542693110191:53)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;init&gt;(command-3848542693110191:57)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;clinit&gt;(command-3848542693110191)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1125)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1078)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:733)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:716)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:64)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:64)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:693)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error 401; url='https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token failed for getting token from AzureAD.; requestId='d2755864-8e28-45e9-b993-af70d74e6b00'; contentType='application/json; charset=utf-8'; response '{&quot;error&quot;:&quot;invalid_client&quot;,&quot;error_description&quot;:&quot;AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '8dcf9602-2f46-4edb-81cf-09318737aef8'.\\r\\nTrace ID: d2755864-8e28-45e9-b993-af70d74e6b00\\r\\nCorrelation ID: 8af9a767-8ca6-4778-b60b-56d117d6a634\\r\\nTimestamp: 2023-02-03 17:19:08Z&quot;,&quot;error_codes&quot;:[7000215],&quot;timestamp&quot;:&quot;2023-02-03 17:19:08Z&quot;,&quot;trace_id&quot;:&quot;d2755864-8e28-45e9-b993-af70d74e6b00&quot;,&quot;correlation_id&quot;:&quot;8af9a767-8ca6-4778-b60b-56d117d6a634&quot;,&quot;error_uri&quot;:&quot;https://login.microsoftonline.com/error?code=7000215&quot;}'\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:430)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:306)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:287)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:110)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:1231)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:319)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:267)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:207)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:205)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:385)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1241)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1208)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1166)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:623)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:97)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$3(DBUtilsCore.scala:218)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$2(DBUtilsCore.scala:215)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:139)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:209)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:1)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:43)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:45)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:47)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw.&lt;init&gt;(command-3848542693110191:49)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw.&lt;init&gt;(command-3848542693110191:51)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read.&lt;init&gt;(command-3848542693110191:53)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;init&gt;(command-3848542693110191:57)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;clinit&gt;(command-3848542693110191)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1125)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1078)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:733)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:716)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:64)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:64)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:693)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)</div>","errorSummary":"AbfsRestOperationException: HTTP Error 401; url='https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token failed for getting token from AzureAD.; requestId='d2755864-8e28-45e9-b993-af70d74e6b00'; contentType='application/json; charset=utf-8'; response '{\"error\":\"invalid_client\",\"error_description\":\"AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '8dcf9602-2f46-4edb-81cf-09318737aef8'.\\r\\nTrace ID: d2755864-8e28-45e9-b993-af70d74e6b00\\r\\nCorrelation ID: 8af9a767-8ca6-4778-b60b-56d117d6a634\\r\\nTimestamp: 2023-02-03 17:19:08Z\",\"error_codes\":[7000215],\"timestamp\":\"2023-02-03 17:19:08Z\",\"trace_id\":\"d2755864-8e28-45e9-b993-af70d74e6b00\",\"correlation_id\":\"8af9a767-8ca6-4778-b60b-56d117d6a634\",\"error_uri\":\"https://login.microsoftonline.com/error?code=7000215\"}'\nCaused by: AzureADAuthenticator.HttpException: HTTP Error 401; url='https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token failed for getting token from AzureAD.; requestId='d2755864-8e28-45e9-b993-af70d74e6b00'; contentType='application/json; charset=utf-8'; response '{\"error\":\"invalid_client\",\"error_description\":\"AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '8dcf9602-2f46-4edb-81cf-09318737aef8'.\\r\\nTrace ID: d2755864-8e28-45e9-b993-af70d74e6b00\\r\\nCorrelation ID: 8af9a767-8ca6-4778-b60b-56d117d6a634\\r\\nTimestamp: 2023-02-03 17:19:08Z\",\"error_codes\":[7000215],\"timestamp\":\"2023-02-03 17:19:08Z\",\"trace_id\":\"d2755864-8e28-45e9-b993-af70d74e6b00\",\"correlation_id\":\"8af9a767-8ca6-4778-b60b-56d117d6a634\",\"error_uri\":\"https://login.microsoftonline.com/error?code=7000215\"}'","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:338)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:267)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:207)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:205)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:385)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1241)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1208)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1166)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:623)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:97)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$3(DBUtilsCore.scala:218)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$2(DBUtilsCore.scala:215)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:139)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:209)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:1)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:43)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:45)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:47)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw.&lt;init&gt;(command-3848542693110191:49)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw.&lt;init&gt;(command-3848542693110191:51)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read.&lt;init&gt;(command-3848542693110191:53)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;init&gt;(command-3848542693110191:57)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;clinit&gt;(command-3848542693110191)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1125)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1078)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:733)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:716)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:64)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:64)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:693)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error 401; url='https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/f94768c8-8714-4abe-8e2d-37a64b18216a/oauth2/token failed for getting token from AzureAD.; requestId='d2755864-8e28-45e9-b993-af70d74e6b00'; contentType='application/json; charset=utf-8'; response '{&quot;error&quot;:&quot;invalid_client&quot;,&quot;error_description&quot;:&quot;AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app '8dcf9602-2f46-4edb-81cf-09318737aef8'.\\r\\nTrace ID: d2755864-8e28-45e9-b993-af70d74e6b00\\r\\nCorrelation ID: 8af9a767-8ca6-4778-b60b-56d117d6a634\\r\\nTimestamp: 2023-02-03 17:19:08Z&quot;,&quot;error_codes&quot;:[7000215],&quot;timestamp&quot;:&quot;2023-02-03 17:19:08Z&quot;,&quot;trace_id&quot;:&quot;d2755864-8e28-45e9-b993-af70d74e6b00&quot;,&quot;correlation_id&quot;:&quot;8af9a767-8ca6-4778-b60b-56d117d6a634&quot;,&quot;error_uri&quot;:&quot;https://login.microsoftonline.com/error?code=7000215&quot;}'\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:430)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:306)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:287)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:110)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:1231)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:319)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:267)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:207)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:205)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:385)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1241)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1208)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:1166)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:623)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:97)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:562)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:94)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$3(DBUtilsCore.scala:218)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$2(DBUtilsCore.scala:215)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:139)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:639)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:634)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:553)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:544)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:514)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:209)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:1)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:43)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:45)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw$$iw.&lt;init&gt;(command-3848542693110191:47)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw$$iw.&lt;init&gt;(command-3848542693110191:49)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$$iw.&lt;init&gt;(command-3848542693110191:51)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read.&lt;init&gt;(command-3848542693110191:53)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;init&gt;(command-3848542693110191:57)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$read$.&lt;clinit&gt;(command-3848542693110191)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec54a2bc6390d45aab7bdba68224e4c7925.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1125)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1078)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:733)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:716)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:401)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:399)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:64)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:444)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:429)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:64)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:693)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:750)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["const_staging_path = \"/mnt/data-source/TwitterDataJsonSource/\"\n# Bronze Table Setup\n@dlt.table(\n    comment=\"Raw data\",\n    table_properties={\n    \"quality\": \"bronze\"\n    }    \n)\n@dlt.expect_or_drop(\"valid_city\", \"City IS NOT NULL\")\ndef bronze_twitter_historical_data():\n        return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .schema(twitterSchema)\n      .load(const_staging_path)\n  )\n\n\n# Silver Table Setup\n@dlt.create_table(\n  comment=\"Preparing\",  \n  table_properties={\n    \"quality\": \"silver\"\n  }    \n)\ndef silver_twitter_historical_data():\n  return dlt.readStream(\"bronze_twitter_historical_data\").filter(col(\"tweet\").contains(\"#fashion\") | col(\"tweet\").contains(\"#beach\")| col(\"tweet\").contains(\"#entertainment\")| col(\"tweet\").contains(\"#gogreen\")| col(\"tweet\").contains(\"#sustainablefashion\")| col(\"tweet\").contains(\"#futuretech\")).withColumn(\"tweet\", clean_twitter_text_udf('tweet')).withColumn(\"Date\",to_date(\"time\"))\n\n\n\n# Gold Table Setup\n@dlt.create_table(\n  comment=\"Predicting\", \n  partition_cols = [\"Date\"],\n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_data():\n  return dlt.read(\"silver_twitter_historical_data\").withColumn('MLSentiment', loaded_model('tweet'))\n\n\n\n# Gold (Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_hashtag_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"hashtag\").count()\n\n\n\n# Gold (Retweet) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_retweetcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").selectExpr(\"sum(cast(retweetcount as int)) RetweetCount\")\n\n\n\n# Gold (City vs Hashtag) Table Setup\n@dlt.create_table(\n  comment=\"Aggregating\",  \n  table_properties={\n    \"quality\": \"gold\"\n  }\n)\ndef gold_twitter_historical_city_hashtagcount_data():\n  return dlt.read(\"silver_twitter_historical_data\").groupBy(\"city\", \"hashtag\").count()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a22fb995-76bc-42ad-8151-240361dae8c4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Campaign Analytics<br/>\n\n1. **Usecase               :** Performing Campaign analytics on static campaign data coming from snowflake in azure container.<br/>\n2. **Notebook Summary      :** This notebook is a part of campaign analytics application which perform `campaign analytics using various pyspark capability`.<br/>\n3. **Notebook Description  :** Performing Campaign Analytics on Azure Container Files.\n\n\n###Feature List\n1. Data Profiling\n2. Total Revenue, Total Revenue Target & Profit \n3. Campaign Run by Per Week \n4. Total Profit by Country Per Week\n5. Top Loss-Making Campaign \n\nThe bronze data received for processing is already curated. So, we will derive gold tables from bronze tables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed012a04-7534-4173-804b-aba623abbb86","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"408ffa24-722e-4470-a065-1e819d76a5b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum as _sum\nfrom pyspark.sql.functions import mean as _mean\nfrom pyspark.sql.functions import max as _max\nfrom pyspark.sql.functions import min as _min\nimport pyspark.sql.functions as func\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b52b9012-41bf-4411-a979-0bfa987e3360","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Define the Schema for the input file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4d68142-3f87-4959-b024-3ae7bacfba8d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["campaignSchema = StructType([    \n    StructField(\"Region\",StringType(),True),\n    StructField(\"Country\",StringType(),True),\n    StructField(\"ProductCategory\",StringType(),True),\n    StructField(\"Campaign_ID\",IntegerType(),True),    \n    StructField(\"Campaign_Name\",StringType(),True),\n    StructField(\"Qualification\",StringType(),True),\n    StructField(\"Qualification_Number\",StringType(),True),\n    StructField(\"Response_Status\",StringType(),True),\n    StructField(\"Responses\",FloatType(),True),\n    StructField(\"Cost\",FloatType(),True),\n    StructField(\"Revenue\",FloatType(),True),\n    StructField(\"ROI\",FloatType(),True),\n    StructField(\"Lead_Generation\",StringType(),True),\n    StructField(\"Revenue_Target\",FloatType(),True),\n    StructField(\"Campaign_Tactic\",StringType(),True),\n    StructField(\"Customer_Segment\",StringType(),True),\n    StructField(\"Status\",StringType(),True),\n    StructField(\"Profit\",FloatType(),True),\n    StructField(\"Marketing_Cost\",FloatType(),True),\n    StructField(\"CampaignID\",IntegerType(),True),\n    StructField(\"CampDate\",DateType(),True), \n    StructField(\"SORTED_ID\",IntegerType(),True)])\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d45d10c6-7106-4c1e-aa4e-2ca376094921","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Load the Campaign Dataset from Data Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"031c2e41-89e5-402c-b694-9781fc3f9925","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Bronze Table Setup\n@dlt.table(comment=\"Raw data\")\ndef bronze_campaign_data():\n#   return (spark.table(\"campaign.campaign_source\"))\n  return (spark.read.format(\"csv\").option(\"header\",True).schema(campaignSchema).load(\"/mnt/data-source/Campaign Data/campaign-data2.csv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16002a35-d24b-451f-89ce-052e751d0ed0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Total Revenue, Total Revenue Target & Profit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f30b9e7-33aa-47d7-a4f7-380ee4228d3b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_country_wise_revenue():\n    \n    df = dlt.read(\"bronze_campaign_data\").groupBy(\"Country\",\"Region\").agg(_sum(\"Revenue\").alias(\"Total_Revenue\"), _sum(\"Revenue_Target\").alias(\"Total_Revenue_Target\"),_sum(\"Profit\").alias(\"Total_Profit\"),_max(\"Cost\").alias(\"Max_Cost\"),_min(\"Cost\").alias(\"Min_Cost\"))\n    df = df.withColumn(\"Total_Revenue\", func.round(df[\"Total_Revenue\"],2)).withColumn(\"Total_Revenue_Target\", func.round(df[\"Total_Revenue_Target\"], 2)).withColumn(\"Total_Profit\", func.round(df[\"Total_Profit\"], 2))\n    return df\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e87b998-2b5e-4c3b-9390-03341e084dc0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Campaign Run by Per Week"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f10df25-706d-46b3-b7b5-0e55087c85ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_campaign_per_week():\n    return dlt.read(\"bronze_campaign_data\") \\\n    .groupBy(\n      \"Campaign_Name\",\n      window(\"CampDate\", \"1 week\")) \\\n    .count().orderBy(col(\"count\").desc())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a43b8ba2-12f6-4003-9987-eb9b19dd2dc3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Total Profit by Country Per Week"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df5e23ec-7078-46b9-ac84-ce39d34100b0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_Total_Profit_by_Country_Per_Week():\n    return dlt.read(\"bronze_campaign_data\").select(\"Region\",\"Country\", \"Cost\", \"Profit\",\"CampDate\") \\\n                     .groupBy(window(col(\"CampDate\"), \"7 days\"), col(\"Country\")) \\\n                     .agg(sum(\"Profit\").alias('Total_Profit'),) \\\n                            .orderBy(col(\"window.start\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c2d981d-ea18-422d-8cf8-580f1467269b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Top Loss-Making Campaign"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5df18a44-6873-46d0-b38b-fc4173bfd985","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Gold Table Setup\n@dlt.table(comment=\"Raw data\")\ndef gold_Top_Loss_Making_Campaign():\n    loss = dlt.read(\"bronze_campaign_data\").select(\"Campaign_Name\",\"Profit\").filter(F.col(\"Profit\") < 0)\n    loss = loss.withColumn(\"Loss_Count\", F.when((F.col('Profit') < 0 ) , F.lit(1)).otherwise(F.lit(0)))\n#     loss = loss.groupBy('Campaign_Name').sum('Loss_Count')\n    return loss"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54eb6e72-5953-4088-ad5b-b38859df96e2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Campaign Powered by Twitter"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48430a0c-b2a4-4688-a0a1-d5e8cc610f86","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["   \n@dlt.table(\n    comment=\"Campaign powered by Twitter\",\n    table_properties={\n    \"quality\": \"gold\"\n    }    \n) \n\ndef Sentiment_Campaign_Analytics():\n    return (\n        spark.sql(\"\"\"\n         select a.* from live.gold_twitter_historical_city_hashtagcount_data a,\n              live.gold_campaign_per_week b\n    where replace(a.hashtag, '#', '') = b.campaign_name\n        \"\"\")\n    )\n    \n\n       "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68c1e285-1a5d-4221-a2fc-0d68cce3c39c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#  Retail Sales Data Preparation using Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"42fd4724-7ec9-4b05-b3fb-4b13f42c1014","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Preparing retail data for training a regression model to predict total sales revenue of a product from a store using the following features: \n- Brand (The brand of the product)\n- Quantity (Quantity of product purchased)\n- Advert (Whether the product had an advertisement or not)\n- Price (How much the product costs)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a67036be-0c8b-4d04-88c9-040e4df43d17","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<div><img src=\"https://stanalyticssolutionsdev.blob.core.windows.net/assets/sales_forecasting.jpg?sp=r&st=2022-09-23T16:12:34Z&se=2025-01-01T01:12:34Z&spr=https&sv=2021-06-08&sr=b&sig=l8Prl1UTwclNsUJQhhCKGxL%2B21dGPvUQVJKnEpB0NRk%3D\" width=\"500\" height=\"300\"/></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46cdcc1a-a952-4777-b4dd-98dc96fe40b9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Importing Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0160b840-0813-4a05-b06f-e9b28c226739","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import percent_rank\nfrom pyspark.sql import Window\nfrom io import BytesIO\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom dateutil import parser\nimport logging\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa90b8e7-915d-48d7-a8b4-98483cba54b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Defining the schema for the data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7463b60-641d-4c38-bd7f-b517229ccc0e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Dataschema = StructType([\n    StructField(\"ID\", StringType()),\n    StructField(\"WeekStarting\", DateType()),\n    StructField(\"Store\", IntegerType()),\n    StructField(\"Brand\", StringType()),\n    StructField(\"Quantity\", IntegerType()),\n    StructField(\"Advert\", IntegerType()),\n    StructField(\"Price\", FloatType()),\n    StructField(\"Revenue\", FloatType())\n])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37ef2fbc-ddf8-4516-a0ba-372a43f0b252","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load the data from the source and perform the transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c6b3278-32ee-49b5-abfc-3165eceddbfb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@dlt.table(comment=\"Raw data\")\ndef bronze_SalesTrans():\n  return (spark.read.csv('/mnt/data-source/Store Transactions Data/dbo.SalesTransData.txt',schema=Dataschema))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f802d99-1735-44ba-a5a2-e8f76fc5a8c5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Silver data\")\ndef silver_rank_data():\n    pydf = dlt.read('bronze_SalesTrans').withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"WeekStarting\")))\n    return pydf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dca9fb50-41ac-403d-82de-2d562c23da65","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@dlt.table(comment=\"Gold data\")\ndef gold_train():\n    train = dlt.read('silver_rank_data').where(\"rank <= .8\").drop(\"rank\")\n    return train\n    \n@dlt.table(comment=\"Gold data\")\ndef gold_test():\n    test = dlt.read('silver_rank_data').where(\"rank > .8\").drop(\"rank\")\n    return test"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9ec0157-70ad-45ac-b89d-20523d7394ed","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DLT notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3848542693110191,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2734317015725973}},"nbformat":4,"nbformat_minor":0}
