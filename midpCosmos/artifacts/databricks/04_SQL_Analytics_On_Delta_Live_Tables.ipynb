{"cells":[{"cell_type":"markdown","source":["#SQL Analytics On Delta Live Tables<br/>\n\n1. **Application           :** Social Media Analytics <br/>\n2. **Usecase               :** Performing SQL analytics and Machine Learning algorithms on Twitter data coming incrementally from data lake.<br/>\n3. **Notebook Summary      :** This notebook is a part of social media analytics application which perform `SQL Analytics`.<br/>\n4. **Notebook Description  :** Performing SQL Analytics on top of Silver layer to derive `Gold layer`, which has aggregated/curated data for visuals and dashboards.\n\n\n###Feature List\n1. Lakehouse Architecture\n2. Batch processing Twitter Messages\n3. Delta Live Tables (Bronze, Silver and Gold and aggregation)\n4. Hashtag Count\n5. Z-Order Optimization \n6. Table Caching\n7. Time Travel"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84ede9fa-721c-4ba7-bbc5-4dbfaa12faf9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install geopy folium"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c2315f0-d26e-4409-bebb-b30104ea2ac0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import requests\nimport base64\nimport mimetypes\n\ndef url_image_to_base64_type_tuple(img_url):\n  \"\"\"\n  Provide a URL to an image and have it converted to base64.\n  - Url e.g.: `https://databricks.com/wp-content/themes/databricks/assets/images/header_logo_2x.png`\n  Returns tuple of base64 data, mime_type\n  \"\"\"\n  # notice the additional decode('utf-8') call\n  response = requests.get(img_url)\n  b64_data = str(base64.b64encode(response.content).decode('utf-8'))\n  return b64_data, response.headers['Content-Type']\n\ndef display_img(base64_data, mime_type, width_percent=None):\n  \"\"\"\n  Use `displayHTML` to display the base64 data using mime type).\n  - If width_percent is provided, will set as attribute on `img` element.\n  \"\"\"\n  if width_percent is None:\n    displayHTML(f\"\"\"<img src=\"data:{mime_type};base64,{base64_data}\">\"\"\") \n  else:\n    displayHTML(f\"\"\"<img src=\"data:{mime_type};base64,{base64_data}\" width=\"{width_percent}%\">\"\"\") \n    \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0707678-bb3f-4f9b-82e0-348f32f7a21c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Raw Twitter Data - Bronze\nOur bronze layer stores the raw, unprocessed data from our Twitter API pulls. By leaving it in its raw state, we give ourselves the option to reprocess it for different purposes in the future. Thanks to Azure Data Lake Gen 2, we can maintain this data for as long as we need it at very low costs. The bronze layer is usually the domain of *data engineers* who then build pipelines to refine this data forward into the silver layer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e91d9ac5-990a-43e4-a2d1-1c1e03bccfbf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSelect Time, Hashtag, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language \nfrom lakedb.bronze_twitter_historical_data\n\n "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94e0ce7a-c82d-4124-ae44-6c5bba790c24","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSelect Time, Hashtag, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language  \nfrom lakedb.bronze_twitter_historical_data\nWhere City <> 'Online'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"031a4955-df02-4057-826c-07ecfb6284d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Filtered Twitter Data - Silver\nIn our silver layer, we've curated our raw Twitter data into something more usable for *data scientists.* They can take these cleaned up, detailed level tables and develop features for machine learning models as well as aggregated analytical datasets for data analysts."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b27b10cd-3103-4875-a2d2-f3d81826e127","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSelect Time, Hashtag, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language\nfrom lakedb.silver_twitter_historical_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9603ddb-8713-4e1a-ac80-84f5b72255d1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Curated Twitter Data - Gold\n\nIn our gold layer, we can enhance and refine our silver data sets even further into fit-for-purpose tables and views for specific analytical needs. Here we've augmented our Twitter data with a machine learning model identifying the sentiment (positive, neutral, or negative) of each Tweet so we can get a sense of how the Twitter tags we're analyzing are being used."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98856846-e008-4337-8461-790ee6070b6f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSelect MLSentiment,Time, Hashtag, RetweetCount, FavouriteCount, IsRetweet, HourOfDay, Language  \nfrom lakedb.gold_twitter_historical_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd401bb2-36f6-499e-a460-84b524936567","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Aggregations\n\nBy pre-emptively aggregating our data that rarely or slowly changes, we can provide a great performance benefit for our end users. Our DLT pipeline performs this aggregation of hashtag counts by the geolocation of the Tweets. By only updating this each time we ingest more Tweets, we can keep the aggregation table up to date and then quickly consume and visualize it in tools like Power BI.\n\nOne nice feature of Databricks notebooks is if a cell produces a DataFrame output (like the one below), you can also profile the data as well as generate quick visualizations. Throw in Markdown and comments and notebooks are a super convenient way to collaborate and communicate with your team, leadership, customers, and other stakeholders."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef58ee6c-3f1c-4ad7-bad2-dbfaf6a24020","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT Hashtag, Count from lakedb.gold_twitter_historical_city_hashtagcount_data where city is not null order by count desc limit 10"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb1ba31c-6370-4c5e-8caf-946d1c131c7d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One way to analyze our Twitter data is by geolocation. We can easily plot our Twitter hashtag counts from above on a fully interactive map visual - directly within our notebook! We could then share this notebook with others so they can explore the location and hashtag data in more detail."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b3366ea-7850-4e17-83ca-47d00f79ca25","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent='twitter-analysis-cl')\n#from pyspark.sql.functions import udf\n\ndef lon_lat(city):\n    try:\n      lon = geolocator.geocode(city)[1][1]\n      lat = geolocator.geocode(city)[1][0]\n      return lat, lon\n    except:\n        return 51.5073219, -0.1276474"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"186d041d-8b89-4423-a142-df22daa10deb","inputWidgets":{},"title":"Define geocoordinate function to get latitude and longitude based on Twitter location"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.sql(\"SELECT * from lakedb.gold_twitter_historical_city_hashtagcount_data where city is not null order by count desc limit 10\")\ndf = df.toPandas()\ndf = df.fillna(\"\")\ndf['latitude'], df['longitude'] = zip(*df.apply(lambda x: lon_lat(x['city']), axis = 1)) #Looking up each city can take awhile! :)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"7164a3ba-3aa1-482d-93a6-63d48c8a8f1a","inputWidgets":{},"title":"Apply geocoordinate function to each city in our dataset"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["hashtagcountdf=spark.createDataFrame(df) \nhashtagcountdf.write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .saveAsTable(\"lakedb.gold_twitter_historical_city_hashtag_count_loc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf05cab1-7bd2-4113-a484-97bf1a02cf8c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT hashtag, count, latitude, longitude from lakedb.gold_twitter_historical_city_hashtag_count_loc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fec7eda-6cfa-4569-a148-85fb38e14ae2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Create world map - Twitter Messages\nMap created below shows the location from where people were tweeting"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3a48499-5279-40d4-8948-37b204fb71bf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import general useful packages\nimport numpy as np\nimport pandas as pd\n\n# Create a world map to show distributions of users \nimport folium\nfrom folium.plugins import MarkerCluster\n\n# Empty map\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\n\n# Creating map\nfor i in range(len(df)):\n  lat = df.iloc[i]['latitude']\n  lon = df.iloc[i]['longitude']\n  radius=5\n  popup_text = \"\"\"Country : {}<br>\"\"\"\n  popup_text = popup_text.format(df.iloc[i]['city'])\n  folium.CircleMarker(location = [lat, lon], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n\n#show the map\nworld_map"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75882e2d-1701-42a4-b1f6-9d28ac6121f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Z-Order Optimization\nZ-Ordering is a technique to colocate related information in the same set of files. <br/> This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms. <br/> This behaviour dramatically reduces the amount of data that Delta Lake on Databricks needs to read"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7258df4b-8be6-4357-b50b-ec6e972f95e5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql \nOPTIMIZE lakedb.gold_twitter_historical_data ZORDER BY city"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45895eab-913c-4914-96b8-24215001d105","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Caching\nCaching reduces scanning of the original files in future queries. It basically caches contents of a table in Apache Spark cache. If a query is cached, then a temp view is created for this query."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95812678-7368-45ae-a509-c720759da9e6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql \nCACHE SELECT * FROM lakedb.gold_twitter_historical_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fb88c56-0b69-4fa2-83c3-bfb9f414230a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Time Travel\nIn audit history above we can view the history of the different versions of the table and load and display any of those versions. <br/>\nIn the exercise below we have displayed data in a specific version"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"337254f7-45db-4edc-abba-2a10d4c3c246","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nhistory = spark.sql(\"DESCRIBE HISTORY lakedb.`gold_twitter_historical_data`\")\nlatest_version = history.selectExpr(\"max(version)\").collect()\nversion = (latest_version[0][0])\nprint(\"Version:\") \nprint(version)\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", version).table(\"lakedb.gold_twitter_historical_data\")\nprint(\"Rows:\") \nprint(df.count())\ndisplay(df.select(col(\"Time\"),col(\"MLSentiment\"),col(\"Hashtag\"),col(\"RetweetCount\"),col(\"FavouriteCount\"),col(\"IsRetweet\"),col(\"HourOfDay\"),col(\"Language\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc97baa8-999d-4982-bce1-5b8cfb556273","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04_SQL_Analytics_On_Delta_Live_Tables.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2734317015725933}},"nbformat":4,"nbformat_minor":0}
